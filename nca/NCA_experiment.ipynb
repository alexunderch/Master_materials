{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexunderch/Master_materials/blob/main/nca/NCA_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgXwZTvrSjeC"
      },
      "source": [
        "## Basic Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN6YfVK0UQyI"
      },
      "source": [
        "The majority of used code is taken from [here](https://distill.pub/2020/growing-ca/) and [here](https://github.com/chenmingxiang110/Growing-Neural-Cellular-Automata/blob/master/lib/CAModel.py). Thank you for the inspiration!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agNOAAPrSJiT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from PIL import Image as pImage, ImageDraw as pImageDraw\n",
        "import io\n",
        "import base64\n",
        "import zipfile\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pylab as pl\n",
        "import glob\n",
        "from copy import deepcopy\n",
        "from IPython.display import Image, HTML, clear_output\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
        "clear_output()\n",
        "\n",
        "from typing import Tuple, Callable\n",
        "import dataclasses\n",
        "from dataclasses import dataclass, field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM0GVPqhT-Gv"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7xI_brtT9TO"
      },
      "outputs": [],
      "source": [
        "#@title Cellular Automata Parameters\n",
        "CHANNEL_N = 16        # Number of CA state channels\n",
        "TARGET_PADDING = 16   # Number of pixels used to pad the target image border\n",
        "TARGET_SIZE = 40\n",
        "BATCH_SIZE = 8\n",
        "POOL_SIZE = 1024\n",
        "CELL_FIRE_RATE = 0.5\n",
        "\n",
        "TARGET_EMOJI = \"ðŸ¦Ž\" #@param {type:\"string\"}\n",
        "\n",
        "EXPERIMENT_TYPE = \"Regenerating\" #@param [\"Growing\", \"Persistent\", \"Regenerating\"]\n",
        "EXPERIMENT_MAP = {\"Growing\": 0, \"Persistent\": 1, \"Regenerating\": 2}\n",
        "EXPERIMENT_N = EXPERIMENT_MAP[EXPERIMENT_TYPE]\n",
        "\n",
        "USE_PATTERN_POOL = [0, 1, 1][EXPERIMENT_N]\n",
        "DAMAGE_N = [0, 0, 3][EXPERIMENT_N]  # Number of patterns to damage in a batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9p2vFxaTNUV"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3S_MZlzTM1i"
      },
      "outputs": [],
      "source": [
        "def np2pil(array: np.ndarray) -> Image:\n",
        "    if array.dtype in [np.float32, np.float64]:\n",
        "        array = np.uint8(np.clip(array, 0, 1) * 255)\n",
        "    return pImage.fromarray(array)\n",
        "\n",
        "def to_rgba(x):\n",
        "    # we have CHANNEL_N by default\n",
        "    return x[..., :4]\n",
        "\n",
        "def to_alpha(x):\n",
        "    return np.clip(x[..., 3:4], a_min = 0., a_max = 0.99999)\n",
        "\n",
        "def to_rgb(x):\n",
        "  # assume rgb premultiplied by alpha\n",
        "  rgb, alpha = x[..., :3], to_alpha(x)\n",
        "  return np.clip(1.0 - alpha + rgb, a_min = 0., a_max = 0.99999)\n",
        "\n",
        "def make_seed(size: int, n: int = 1):\n",
        "    x = np.zeros([n, size, size, CHANNEL_N], np.float32)\n",
        "    x[:, size // 2, size // 2, 3:] = 1.0 \n",
        "    return x\n",
        "\n",
        "### loaders\n",
        "def load_image(url: str, \n",
        "               max_size: int = TARGET_SIZE\n",
        "               ) -> np.ndarray:\n",
        "    r = requests.get(url)\n",
        "    img = pImage.open(io.BytesIO(r.content))\n",
        "    img.thumbnail((max_size, max_size), pImage.ANTIALIAS)\n",
        "    img = np.float32(img) / 255.0\n",
        "    # premultiply RGB by Alpha\n",
        "    alpha  = img[..., 3:]\n",
        "    img[..., :3] *= alpha\n",
        "    return img\n",
        "\n",
        "def load_emoji(emoji) -> np.ndarray:\n",
        "    code = hex(ord(emoji))[2:].lower()\n",
        "    url = 'https://github.com/googlefonts/noto-emoji/blob/main/png/128/emoji_u%s.png?raw=true'%code\n",
        "    return load_image(url)\n",
        "\n",
        "# def load_custom_emoji(index: int, path: str):\n",
        "#     im = imageio.imread(path)\n",
        "#     emoji = np.array(im[:, index*40:(index+1)*40].astype(np.float32))\n",
        "#     emoji /= 255.0\n",
        "#     return emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6L_GGMvTP_r"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7fEtgyNcP69"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFnl56wESwC_"
      },
      "outputs": [],
      "source": [
        "@dataclass(eq = False)\n",
        "class LittleNCA(torch.nn.Module):\n",
        "    def __post_init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    channels: int = field(default = CHANNEL_N)\n",
        "    fire_rate: float =  field(default = CELL_FIRE_RATE)\n",
        "    interm_channels: int = field(default = 128)\n",
        "    \n",
        "    def conv_setup(self, init: bool = False): \n",
        "        model = [\n",
        "                    nn.Conv2d(self.channels * 3, self.interm_channels, 1, padding = 0),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(self.interm_channels, self.channels, 1, padding = 0, bias = False)    \n",
        "                ]\n",
        "        if init:\n",
        "            with torch.no_grad():\n",
        "                model[-1].weight.zero_()\n",
        "        self.model = nn.Sequential(*model)\n",
        "        return self\n",
        "\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def get_living_mask(self, x):\n",
        "        stride_height, stride_width = (1, 1)\n",
        "        out_height, out_width = (1, 1)\n",
        "\n",
        "        alpha = x[:, 3:4, ...]\n",
        "        return F.max_pool2d(\n",
        "                                alpha,\n",
        "                                kernel_size = 3,\n",
        "                                stride = (int(stride_height), int(stride_width)),\n",
        "                                padding = (int(out_height), int(out_width))\n",
        "                            ) > 0.1\n",
        "    @torch.no_grad()    \n",
        "    def perceive(self,\n",
        "                 x: torch.Tensor, \n",
        "                 angle: float = 0.0, \n",
        "                 step_size: float = 1.0):\n",
        "        \n",
        "        identify = np.float32([0, 1, 0])\n",
        "        identify = np.outer(identify, identify)\n",
        "        \"\"\"\n",
        "        identity = \n",
        "                    [[0. 0. 0.]\n",
        "                    [0. 1. 0.]\n",
        "                    [0. 0. 0.]]\n",
        "        \"\"\"\n",
        "        \n",
        "        dx = np.outer([1, 2, 1], [-1, 0, 1]) / 8.0  # Sobel filter\n",
        "        \"\"\"\n",
        "        dx = \n",
        "                    [[-1  0  1]\n",
        "                    [-2  0  2]\n",
        "                    [-1  0  1]]        \n",
        "        \"\"\"\n",
        "        \n",
        "        dy = dx.T\n",
        "        c, s = torch.cos(torch.tensor(angle)), torch.sin(torch.tensor(angle))\n",
        "        \n",
        "        #rotate percieved tensor on `angle`\n",
        "        kernel = torch.stack([torch.Tensor(identify), c * dx - s * dy, s * dx + c * dy], dim = 0)[:, None, ...]\n",
        "        kernel = kernel.repeat(self.channels, 1, 1, 1).to(torch.float32).to(self.device)\n",
        "        kernel.requires_grad_ = False\n",
        "    \n",
        "        return F.conv2d(\n",
        "            input  = x,\n",
        "            weight = kernel,\n",
        "            padding = 1,\n",
        "            groups = self.channels,\n",
        "            stride = (1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, \n",
        "                x: torch.Tensor, \n",
        "                fire_rate: float = None, \n",
        "                angle: float = 0.0, \n",
        "                step_size: float = 1.0):\n",
        "        \n",
        "        if fire_rate is None:\n",
        "            fire_rate = self.fire_rate\n",
        "\n",
        "        x = x.transpose(1, 3)\n",
        "\n",
        "        pre_life_mask = self.get_living_mask(x)\n",
        "        y = self.perceive(x, angle = angle)\n",
        "        dx = self.model(y) * step_size\n",
        "\n",
        "        update_mask = torch.randn_like(x[:, :1, ...]) > fire_rate #> or <= ?\n",
        "        update_mask = update_mask.float().to(self.device)\n",
        "        x +=  dx * update_mask\n",
        "\n",
        "        post_life_mask = self.get_living_mask(x)\n",
        "        life_mask = (pre_life_mask & post_life_mask).float()\n",
        "\n",
        "        out = x * life_mask\n",
        "        return out.transpose(1, 3)\n",
        "\n",
        "    def to_onnx(self, name: str):\n",
        "        torch.onnx.export(self.state_dict(), (72, 72, 16), f'onnx_model_{name}.onnx', verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aojqcmlgh1EV",
        "outputId": "fa5258d0-0989-4326-bce0-b479fb1cec38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 128, 3, 3]           6,272\n",
            "              ReLU-2            [-1, 128, 3, 3]               0\n",
            "            Conv2d-3             [-1, 16, 3, 3]           2,048\n",
            "================================================================\n",
            "Total params: 8,320\n",
            "Trainable params: 8,320\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = LittleNCA()\n",
        "model.conv_setup(init = True)\n",
        "summary(model = model, input_size = (3, 3, 16), batch_size = -1, device = \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am4vfrL-2WXI"
      },
      "source": [
        "## Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNN47OZqkCx5"
      },
      "outputs": [],
      "source": [
        "#copied from the notebook with tf -> torch modifications\n",
        "\n",
        "def imwrite(f, a, fmt=None):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=95)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def im2url(a, fmt='jpeg'):\n",
        "  encoded = imencode(a, fmt)\n",
        "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
        "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
        "\n",
        "def imshow(a, fmt='jpeg'):\n",
        "  display(Image(data=imencode(a, fmt)))\n",
        "\n",
        "def tile2d(a, w=None):\n",
        "  a = np.asarray(a)\n",
        "  if w is None:\n",
        "    w = int(np.ceil(np.sqrt(len(a))))\n",
        "  th, tw = a.shape[1:3]\n",
        "  pad = (w-len(a))%w\n",
        "  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
        "  h = len(a)//w\n",
        "  a = a.reshape([h, w]+list(a.shape[1:]))\n",
        "  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
        "  return a\n",
        "\n",
        "def zoom(img, scale=4):\n",
        "  img = np.repeat(img, scale, 0)\n",
        "  img = np.repeat(img, scale, 1)\n",
        "  return img\n",
        "\n",
        "class VideoWriter:\n",
        "  def __init__(self, filename, fps=30.0, **kw):\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kw)\n",
        "\n",
        "  def add(self, img):\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in [np.float32, np.float64]:\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer:\n",
        "      self.writer.close()\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *kw):\n",
        "    self.close()\n",
        "\n",
        "class SamplePool:\n",
        "  def __init__(self, *, _parent = None, _parent_idx = None, **slots):\n",
        "    self._parent = _parent\n",
        "    self._parent_idx = _parent_idx\n",
        "    self._slot_names = slots.keys()\n",
        "    self._size = None\n",
        "    for k, v in slots.items():\n",
        "      if self._size is None:\n",
        "        self._size = len(v)\n",
        "      assert self._size == len(v)\n",
        "      setattr(self, k, np.asarray(v))\n",
        "\n",
        "  def sample(self, n):\n",
        "    idx = np.random.choice(self._size, n, False)\n",
        "    batch = {k: getattr(self, k)[idx] for k in self._slot_names}\n",
        "    batch = SamplePool(**batch, _parent=self, _parent_idx=idx)\n",
        "    return batch\n",
        "\n",
        "  def commit(self):\n",
        "    for k in self._slot_names:\n",
        "      getattr(self._parent, k)[self._parent_idx] = getattr(self, k)\n",
        "\n",
        "def make_circle_masks(n, h, w):\n",
        "    x = torch.linspace(-1.0, 1.0, w)[None, None, :]\n",
        "    y = torch.linspace(-1.0, 1.0, h)[None, :, None]\n",
        "    center = torch.distributions.uniform.Uniform(-.5 , .5).sample((2, n, 1, 1)) \n",
        "    r = torch.distributions.uniform.Uniform(.1 , .4).sample((n, 1, 1)) \n",
        "    x, y = (x - center[0]) / r, (y - center[1]) / r\n",
        "    mask = x ** 2 + y ** 2 < 1.0\n",
        "    return mask.to(torch.float32).numpy() \n",
        "\n",
        "def generate_pool_figures(pool, step_i):\n",
        "    \n",
        "    tiled_pool = tile2d(to_rgb(pool.x[:49]))\n",
        "    fade = np.linspace(1.0, 0.0, 72)\n",
        "    ones = np.ones(72) \n",
        "    tiled_pool[:, :72] += (-tiled_pool[:, :72] + ones[None, :, None]) * fade[None, :, None] \n",
        "    tiled_pool[:, -72:] += (-tiled_pool[:, -72:] + ones[None, :, None]) * fade[None, ::-1, None]\n",
        "    tiled_pool[:72, :] += (-tiled_pool[:72, :] + ones[:, None, None]) * fade[:, None, None]\n",
        "    tiled_pool[-72:, :] += (-tiled_pool[-72:, :] + ones[:, None, None]) * fade[::-1, None, None]\n",
        "    imwrite('train_log/%04d_pool.jpg'%step_i, tiled_pool)\n",
        "\n",
        "def visualize_batch(x0, x, step_i):\n",
        "    vis0 = np.hstack(to_rgb(x0))\n",
        "    vis1 = np.hstack(to_rgb(x))\n",
        "    vis = np.vstack([vis0, vis1])\n",
        "    imwrite('train_log/batches_%04d.jpg'%step_i, vis)\n",
        "    print('batch (before/after):')\n",
        "    imshow(vis)\n",
        "\n",
        "def plot_loss(loss_log):\n",
        "    pl.figure(figsize=(10, 4))\n",
        "    pl.title('Loss history (log10)')\n",
        "    pl.plot(np.log10(loss_log), '.', alpha=0.1)\n",
        "    pl.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "-ltncTzmGlU2",
        "outputId": "92dfaeaa-1a05-4a36-9452-32dfea62c4eb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Target Emoji\n",
        "target_img = load_emoji(TARGET_EMOJI)\n",
        "imshow(zoom(to_rgb(target_img), 2), fmt='png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNm6Fhps3lgx"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lepJoNFP3k3h",
        "outputId": "4e8ed9c1-d770-4ec4-d848-ae7470a25d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "padded from (40, 40) to (72, 72)\n",
            "device -- cpu -- used\n"
          ]
        }
      ],
      "source": [
        "p = TARGET_PADDING\n",
        "pad_target = np.pad(target_img, [(p, p), (p, p), (0, 0)])\n",
        "print(f\"padded from {target_img.shape[:2]} to {pad_target.shape[:2]}\")\n",
        "\n",
        "h, w = pad_target.shape[:2]\n",
        "seed = np.zeros([h, w, CHANNEL_N], np.float32)\n",
        "seed[h // 2, w // 2, 3:] = 1.0\n",
        "\n",
        "# os.system(\n",
        "#     \"!mkdir -p train_log && rm -f train_log/*\"\n",
        "# )\n",
        "\n",
        "os.makedirs(\"train_log\", exist_ok = True)\n",
        "os.system(\n",
        "     \"rm -f train_log/*\"\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def loss_fn(inp, tgt, reduction: str = \"mean\"):\n",
        "    inp = to_rgba(inp)\n",
        "    if isinstance(inp, np.ndarray):\n",
        "        inp = torch.Tensor(inp)\n",
        "    if isinstance(tgt, np.ndarray):\n",
        "        tgt = torch.Tensor(tgt)\n",
        "    return F.mse_loss(inp, tgt, reduction = \"mean\")\n",
        "\n",
        "@dataclass\n",
        "class NCATrainer:\n",
        "    seed: np.ndarray\n",
        "    tgt_pattern: np.ndarray\n",
        "    device: torch.device\n",
        "    model: torch.nn.Module\n",
        "    n_iterations_bounds: Tuple[int, int] \n",
        "    loss_fn: Callable\n",
        "    optimizer: torch.optim\n",
        "    lr_scheduler: Callable\n",
        "    \n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.model.to(self.device)\n",
        "        self.loss_history = list()\n",
        "        self.pool = SamplePool(x = np.repeat(self.seed[None, ...], POOL_SIZE, 0))\n",
        "        self.tgt_pattern = np.repeat(self.tgt_pattern[None, ...], BATCH_SIZE, 0) \\\n",
        "                           if len(self.tgt_pattern.shape) < 4 else self.tgt_pattern\n",
        "\n",
        "        self.tgt_pattern = torch.from_numpy(self.tgt_pattern.astype(np.float32)).to(self.device)\n",
        "\n",
        "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor]):\n",
        "        n_iterations = int(np.random.randint(*self.n_iterations_bounds))\n",
        "        inp, tgt = batch            \n",
        "\n",
        "        for iter in range(n_iterations):\n",
        "            preds = self.model(inp.to(self.device))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.loss_fn(preds, tgt.to(self.device))\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "        return preds, loss\n",
        "    \n",
        "    def train(self, n_epochs):\n",
        "        for epoch in tqdm(range(n_epochs + 1), total = n_epochs) :\n",
        "            if USE_PATTERN_POOL:\n",
        "                batch = self.pool.sample(BATCH_SIZE)\n",
        "                x0 = batch.x.astype(np.float32)\n",
        "\n",
        "                with torch.no_grad():    \n",
        "                    loss_rank = F.mse_loss(torch.from_numpy(to_rgba(x0)).to(device), \n",
        "                                           self.tgt_pattern, \n",
        "                                           reduction = \"none\").mean((-2, -3, -1))\n",
        "                    loss_rank = loss_rank.detach().cpu().numpy().argsort()[::-1]\n",
        "\n",
        "                x0 = x0[loss_rank]\n",
        "                x0[:1] = self.seed\n",
        "                if DAMAGE_N:\n",
        "                    damage = 1.0 - make_circle_masks(DAMAGE_N, h, w)[..., None]\n",
        "                    x0[-DAMAGE_N:] *= damage\n",
        "            else:\n",
        "                x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0)\n",
        "\n",
        "            x0 = torch.from_numpy(x0.astype(np.float32)).to(device)\n",
        "\n",
        "            x, loss = self.training_step((x0, self.tgt_pattern))\n",
        "            x, x0 = x.detach().cpu().numpy(), x0.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "            if USE_PATTERN_POOL:\n",
        "                batch.x[:] = x\n",
        "                batch.commit()\n",
        "\n",
        "            step_i = len(self.loss_history)\n",
        "            self.loss_history.append(loss.item())\n",
        "\n",
        "            if step_i % 10 == 0:\n",
        "                generate_pool_figures(self.pool, step_i)\n",
        "            if step_i % 100 == 0:\n",
        "                clear_output()\n",
        "                visualize_batch(x0, x, step_i)\n",
        "                plot_loss(self.loss_history)\n",
        "                torch.save(self.model.state_dict(), 'train_log/%04d.pth'%step_i)\n",
        "\n",
        "            print('\\r step: %d, log10(loss): %.3f'%(len(self.loss_history), np.log10(loss.item())), end='')\n",
        "\n",
        "print(f\"device -- {device} -- used\")\n",
        "nca = LittleNCA().conv_setup(init = True).to(device)\n",
        "\n",
        "lr = 2e-3\n",
        "optimizer = torch.optim.Adam(params = nca.parameters(), lr = lr)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer = optimizer, gamma = .1, step_size = 2000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxxcOkHOA4lz"
      },
      "outputs": [],
      "source": [
        "trainer = NCATrainer(\n",
        "    seed = seed,\n",
        "    tgt_pattern = pad_target,\n",
        "    device = device,\n",
        "    model = nca,\n",
        "    n_iterations_bounds = (64, 96),\n",
        "    loss_fn = loss_fn,\n",
        "    optimizer = optimizer,\n",
        "    lr_scheduler = lr_scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "AwPIPNqD9tNx",
        "outputId": "83086b74-e1cb-4fce-b773-a4d8cbcc9ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch (before/after):\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " step: 345, log10(loss): -1.516"
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "trainer.train(20_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2tCvL8ri_L2"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt5Cadj-rlaH"
      },
      "outputs": [],
      "source": [
        "#@title Training Progress (Batches)\n",
        "frames = sorted(glob.glob('train_log/batches_*.jpg'))\n",
        "mvp.ImageSequenceClip(frames, fps=10.0).write_videofile('batches.mp4')\n",
        "mvp.ipython_display('batches.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cju1tfheQ71h"
      },
      "outputs": [],
      "source": [
        "#@title Pool Contents\n",
        "frames = sorted(glob.glob('train_log/*_pool.jpg'))[-80:]\n",
        "mvp.ImageSequenceClip(frames, fps=20.0).write_videofile('pool.mp4')\n",
        "mvp.ipython_display('pool.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VleRKE4pe3IV"
      },
      "outputs": [],
      "source": [
        "#@title Training Progress (Checkpoints)\n",
        "\n",
        "models = []\n",
        "for i in [100, 500, 1000, 4000]:\n",
        "  ca = CAModel()\n",
        "  ca.load_weights('train_log/%04d'%i)\n",
        "  models.append(ca)\n",
        "\n",
        "out_fn = 'train_steps_damage_%d.mp4'%DAMAGE_N\n",
        "x = np.zeros([len(models), 72, 72, CHANNEL_N], np.float32)\n",
        "x[..., 36, 36, 3:] = 1.0\n",
        "with VideoWriter(out_fn) as vid:\n",
        "  for i in tqdm.trange(500):\n",
        "    vis = np.hstack(to_rgb(x))\n",
        "    vid.add(zoom(vis, 2))\n",
        "    for ca, xk in zip(models, x):\n",
        "      xk[:] = ca(xk[None,...])[0]\n",
        "mvp.ipython_display(out_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdJiNrrngEPk"
      },
      "outputs": [],
      "source": [
        "#@title Converting model to `onnx`-compatible format \n",
        "nca.to_onnx()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}