{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75487f87",
      "metadata": {
        "id": "75487f87"
      },
      "source": [
        "## Метод Option-Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "056a611c",
      "metadata": {
        "id": "056a611c"
      },
      "source": [
        "Статья по [Option-Critic архитектуре](https://ojs.aaai.org/index.php/AAAI/article/download/10916/10775).\n",
        "\n",
        "Общая архитектура агента:\n",
        "\n",
        "![image.png](https://d3i71xaburhd42.cloudfront.net/15b26d8cb35d7e795c8832fe08794224ee1e9f84/3-Figure1-1.png)\n",
        "\n",
        "\n",
        "Общий вид алгоритма:\n",
        "\n",
        "![image.png](https://campusai.github.io/_papers/the_option_critic_architecture/algo1optioncritic.png)\n",
        "\n",
        "Good implementation [link](https://github.com/lweitkamp/option-critic-pytorch/blob/master/option_critic.py)! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "daf5568e-cfbe-494a-88ad-27a02bb03800",
      "metadata": {
        "tags": [],
        "id": "daf5568e-cfbe-494a-88ad-27a02bb03800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ab5cb3-edfd-4ed8-83bd-1a84ca6f5fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 1,086 kB of archives.\n",
            "After this operation, 5,413 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1,081 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5,528 B]\n",
            "Fetched 1,086 kB in 1s (1,323 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 122545 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.1-5build1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\n",
            "Unpacking swig (4.0.1-5build1) ...\n",
            "Setting up swig4.0 (4.0.1-5build1) ...\n",
            "Setting up swig (4.0.1-5build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !apt install swig\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license, box2d]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d203f49b",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "d203f49b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1919128-dab1-4e39-fa3e-226641c3616d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical, Bernoulli\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb2b866",
      "metadata": {
        "id": "9eb2b866"
      },
      "source": [
        "### Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "f1a65620",
      "metadata": {
        "tags": [],
        "id": "f1a65620"
      },
      "outputs": [],
      "source": [
        "def print_mean_reward(step, episode_rewards):\n",
        "    if not episode_rewards:\n",
        "        return\n",
        "\n",
        "    t = min(50, len(episode_rewards))    \n",
        "    mean_reward = sum(episode_rewards[-t:]) / t\n",
        "    print(f\"step: {str(step).zfill(6)}, mean reward: {mean_reward:.2f}\")\n",
        "    return mean_reward\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    return x\n",
        "\n",
        "\n",
        "def to_np(t):\n",
        "    return t.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def softmax(x: np.ndarray, temp=1.) -> np.ndarray:\n",
        "    \"\"\"Computes softmax values for a vector `x` with a given temperature.\"\"\"\n",
        "    temp = np.clip(temp, 1e-5, 1e+3)\n",
        "    e_x = np.exp((x - np.max(x, axis=-1)) / temp)\n",
        "    return e_x / e_x.sum(axis=-1)\n",
        "\n",
        "\n",
        "def run(\n",
        "        env: gym.Env, hidden_size: int, n_options: int,\n",
        "        softmax_temp: float, ac_lr: float, cr_lr: float, gamma: float,\n",
        "        termination_regularizer: float, entropy_weight: float,\n",
        "        max_episodes: int, replay_buffer_size: int, update_schedule: int,\n",
        "        batch_size: int, critic_batch_size: int, critic_updates_per_actor: int,\n",
        "        seed: int, print_schedule: int, success_return: float\n",
        "):\n",
        "    # Инициализируйте агента `agent`, когда сделаете саму реализацию агента ниже по заданию.\n",
        "    ####### Здесь ваш код ########\n",
        "    try:\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "    except IndexError:\n",
        "        state_dim = env.observation_space.n\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent =  OptionCriticAgent(state_dim, hidden_size, action_dim, n_options, softmax_temp, \n",
        "                                gamma, ac_lr, cr_lr, termination_regularizer, entropy_weight,\n",
        "                                replay_buffer_size, seed)\n",
        "    ##############################\n",
        "\n",
        "    episode_rewards = []\n",
        "    step, rollout_step = 0, 0\n",
        "    for i_episode in range(1, max_episodes + 1):\n",
        "        cumulative_reward = 0\n",
        "        terminated = False\n",
        "        state, _ = env.reset()\n",
        "        \n",
        "        while not terminated:\n",
        "            step += 1\n",
        "\n",
        "            action, option = agent.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            \n",
        "            agent.append_to_replay_buffer(state, action, option, reward, next_state, terminated)\n",
        "            state = next_state\n",
        "            cumulative_reward += reward\n",
        "            terminated |= truncated\n",
        "            if step % update_schedule == 0:\n",
        "                agent.update(batch_size, critic_batch_size, critic_updates_per_actor)\n",
        "\n",
        "        episode_rewards.append(cumulative_reward)\n",
        "        \n",
        "        # выполняем обновление\n",
        "        if i_episode % print_schedule == 0:\n",
        "            mean_reward = print_mean_reward(step, episode_rewards) \n",
        "            if mean_reward >= success_return:\n",
        "                print('Accepted!')\n",
        "                return agent\n",
        "            episode_rewards = []\n",
        "            \n",
        "    return agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "bebcc19f",
      "metadata": {
        "tags": [],
        "id": "bebcc19f"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "from operator import attrgetter\n",
        "\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'option', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size, seed):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "        self._rng = np.random.default_rng(seed)\n",
        "    \n",
        "    def append(self, state, action, option, reward, next_state, done):\n",
        "        sample = Transition(state, action, option, reward, next_state, done)\n",
        "        self.buffer.append(sample)\n",
        "        \n",
        "    def get_last_n_samples(self, n_samples):\n",
        "        # Get last `n_samples` samples from replay buffer\n",
        "        buffer_size = len(self.buffer)\n",
        "        if buffer_size < n_samples:\n",
        "            return None\n",
        "\n",
        "        ####### Здесь ваш код ########\n",
        "        indices = np.arange(buffer_size-n_samples, buffer_size)\n",
        "        ##############################\n",
        "        return self._get_batch(indices)\n",
        "\n",
        "    def sample_batch(self, n_samples):\n",
        "        buffer_size = len(self.buffer)\n",
        "        if buffer_size < n_samples:\n",
        "            return None\n",
        "        \n",
        "        ####### Здесь ваш код ########\n",
        "        indices = np.random.choice(len(self.buffer), n_samples, replace=False)\n",
        "        ##############################\n",
        "\n",
        "        return self._get_batch(indices)\n",
        "\n",
        "    def _get_batch(self, indices):\n",
        "        states, actions, options, rewards, next_states, dones = [], [], [], [], [], []\n",
        "        for i in indices:\n",
        "            s, a, o, r, n_s, done = self.buffer[i]\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            options.append(o)\n",
        "            rewards.append(r)\n",
        "            next_states.append(n_s)\n",
        "            dones.append(done)\n",
        "\n",
        "        batch = (\n",
        "            np.array(states), np.array(actions), np.array(options), \n",
        "            np.array(rewards), np.array(next_states), np.array(dones)\n",
        "        )\n",
        "        return batch\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b22de37e",
      "metadata": {
        "tags": [],
        "id": "b22de37e"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = to_tensor(state)\n",
        "        return self.net(state)\n",
        "\n",
        "class OptionCriticNet(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, action_dim, num_options, softmax_temp):\n",
        "        super(OptionCriticNet, self).__init__()\n",
        "\n",
        "        self.body = MLPModel(state_dim, hidden_dim)\n",
        "        self.q = nn.Linear(hidden_dim, num_options)\n",
        "        self.pi = nn.Linear(hidden_dim, num_options * action_dim)\n",
        "        self.beta = nn.Linear(hidden_dim, num_options) #options terminantion\n",
        "        \n",
        "        self.num_options = num_options\n",
        "        self.action_dim = action_dim\n",
        "        self.softmax_temp = softmax_temp\n",
        "\n",
        "\n",
        "    @torch.no_grad()   \n",
        "    def predict_option_temination(self, state, option):\n",
        "        beta = self.beta(state)[:, option].sigmoid() \n",
        "        option_termination = bool(Bernoulli(beta).sample().item())\n",
        "        next_option = self.q_values(state).argmax(-1).item()\n",
        "        return option_termination, next_option\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.body(state)\n",
        "\n",
        "        ####### Здесь ваш код ########\n",
        "        q_values = self.q(x)\n",
        "        beta = self.beta(x).sigmoid() \n",
        "        pi = self.pi(x).view(-1, self.num_options, self.action_dim).squeeze(0)\n",
        "\n",
        "        pi = (pi / self.softmax_temp).softmax(-1)\n",
        "        log_pi = (pi / self.softmax_temp).log_softmax(-1)    \n",
        "\n",
        "        ##############################\n",
        "        \n",
        "        return {\n",
        "            'q': q_values,\n",
        "            'beta': beta,\n",
        "            'pi': pi,\n",
        "            'log_pi': log_pi,\n",
        "        }\n",
        "\n",
        "class OptionCriticAgent:\n",
        "    def __init__(\n",
        "        self, state_dim, hidden_dim, action_dim, num_options, softmax_temp, \n",
        "        gamma, ac_lr, cr_lr, termination_regularizer, entropy_weight,\n",
        "        replay_buffer_size, seed\n",
        "    ):\n",
        "        self.network = OptionCriticNet(state_dim, hidden_dim, action_dim, num_options, softmax_temp).to(device)\n",
        "        self.cr_optimizer = torch.optim.Adam(self.network.parameters(), lr=ac_lr)\n",
        "        self.ac_optimizer = torch.optim.Adam(self.network.parameters(), lr=cr_lr)\n",
        "\n",
        "        self.n_options = num_options\n",
        "        self.n_actions = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.softmax_temp = softmax_temp\n",
        "        self.termination_regularizer = termination_regularizer\n",
        "        self.entropy_weight = entropy_weight\n",
        "        \n",
        "        self.replay_buffer = ReplayBuffer(replay_buffer_size, seed)\n",
        "\n",
        "        self._rng = np.random.default_rng(seed)\n",
        "        self.option = None\n",
        "\n",
        "    @torch.no_grad()    \n",
        "    def act(self, state):\n",
        "        ####### Здесь ваш код ########\n",
        "        preds = self.network(to_tensor(state))\n",
        "        \n",
        "        self.option = self.sample_option(\n",
        "            q_options=preds['q'],\n",
        "            beta_options=preds['beta']\n",
        "        )\n",
        "        action_dist = Categorical(probs=preds['pi'][self.option])\n",
        "        action = action_dist.sample().item()\n",
        "\n",
        "        ##############################\n",
        "        option = self.option\n",
        "        return action, option\n",
        "    \n",
        "    def update(self, batch_size, critic_batch_size, critic_updates_per_actor):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return False\n",
        "        \n",
        "        self.update_actor(batch_size)\n",
        "        self.update_critic(critic_batch_size, critic_updates_per_actor)\n",
        "        return True\n",
        "\n",
        "    def update_actor(self, batch_size):\n",
        "        batch = self.replay_buffer.get_last_n_samples(batch_size)\n",
        "        if not batch:\n",
        "            return\n",
        "        \n",
        "        states, actions, options, rewards, next_states, is_done = batch\n",
        "        \n",
        "        ####### Здесь ваш код ########\n",
        "        s = to_tensor(states)                       \n",
        "        a = to_tensor(actions, int).long().unsqueeze(-1)          \n",
        "        o = to_tensor(options, int).long().unsqueeze(-1)       \n",
        "        r = to_tensor(rewards)                       \n",
        "        s_n = to_tensor(next_states)                \n",
        "        not_done = 1 - to_tensor(is_done, int)\n",
        "        \n",
        "        preds = self.network(s)\n",
        "        # q: torch.Size([200, 4])\n",
        "        # beta: Size([200, 4])\n",
        "        # pi: Size([200, 4, 2])\n",
        "        # log_pi: Size([200, 4, 2])\n",
        "\n",
        "        options_for_actions = o.unsqueeze(-1).expand(*o.size(), self.n_actions)\n",
        "        pi_o = preds['pi'].gather(1, options_for_actions).squeeze(1)\n",
        "        log_pi_o = preds['log_pi'].gather(1, options_for_actions).squeeze(1)\n",
        "        log_pi_o_for_actions = log_pi_o.gather(1, a).squeeze(-1)\n",
        "        beta_o = preds['beta'].gather(1, o).squeeze(-1)\n",
        "        \n",
        "        q_values = preds['q'].detach()\n",
        "        v_s = q_values.max(-1)[0]\n",
        "\n",
        "        q_under_options = q_values.gather(1, o).squeeze(-1)\n",
        "        beta_advantage = q_under_options - v_s\n",
        "        beta_advantage  = (beta_advantage - beta_advantage.mean()) / (beta_advantage.std() + 1e-7)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds_next = self.network(s_n)\n",
        "            beta_o_prime = preds_next['beta'].gather(1, o).squeeze(-1)\n",
        "            q_values_next_options = preds_next['q'].gather(1, o).squeeze(-1)\n",
        "            v_s_prime = preds_next['q'].max(-1)[0]\n",
        "\n",
        "        utility_upon_arrival =  ((1 - beta_o_prime) * q_values_next_options + beta_o_prime * v_s_prime)\n",
        "        gt = r + not_done * self.gamma * utility_upon_arrival\n",
        "        entropy = -self.entropy_weight * (log_pi_o * pi_o).sum(-1)\n",
        "\n",
        "        beta_loss = (beta_o * beta_advantage + self.termination_regularizer) * not_done\n",
        "        policy_loss = -log_pi_o_for_actions * (gt.detach() - q_under_options) + entropy\n",
        "        \n",
        "        self.ac_optimizer.zero_grad()\n",
        "        loss = (policy_loss + beta_loss).mean()\n",
        "        loss.backward()\n",
        "        self.ac_optimizer.step()\n",
        "\n",
        "        ##############################\n",
        "    \n",
        "    def update_critic(self, batch_size, critic_updates_per_actor):\n",
        "        # ограничивает сверху количество эпох для буфера небольшого размера\n",
        "        critic_updates_per_actor = min(\n",
        "            critic_updates_per_actor, \n",
        "            5 * len(self.replay_buffer.buffer) // batch_size\n",
        "        )\n",
        "        \n",
        "        for _ in range(critic_updates_per_actor):\n",
        "            self.update_critic_step(batch_size)\n",
        "\n",
        "\n",
        "    def update_critic_step(self, batch_size):\n",
        "        batch = self.replay_buffer.sample_batch(batch_size)\n",
        "        if not batch:\n",
        "            return\n",
        "        \n",
        "        states, actions, options, rewards, next_states, is_done = batch\n",
        "        \n",
        "        # Реализуйте шаг обучения критика\n",
        "        ####### Здесь ваш код ########\n",
        "        s = to_tensor(states)                       \n",
        "        a = to_tensor(actions, int).long()          \n",
        "        o = to_tensor(options, int).long().unsqueeze(-1)          \n",
        "        r = to_tensor(rewards)                       \n",
        "        s_n = to_tensor(next_states)                \n",
        "        not_done = 1 - to_tensor(is_done, int)               \n",
        "        \n",
        "        preds = self.network(s)\n",
        "        \n",
        "        q_under_options = preds['q'].gather(1, o).squeeze(-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds_next = self.network(s_n)\n",
        "            beta_o_prime = preds_next['beta'].gather(1, o).squeeze(-1)\n",
        "            q_values_next_options = preds_next['q'].gather(1, o).squeeze(-1)\n",
        "            v_s_prime = preds_next['q'].max(-1)[0]\n",
        "\n",
        "        utility_upon_arrival = ((1 - beta_o_prime) * q_values_next_options + beta_o_prime * v_s_prime)\n",
        "        td_target = r + not_done * self.gamma * utility_upon_arrival\n",
        "\n",
        "        td_error = q_under_options - td_target.detach()\n",
        "\n",
        "        loss = td_error.pow(2).mean()\n",
        "\n",
        "        self.cr_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.cr_optimizer.step()\n",
        "\n",
        "        ##############################\n",
        "\n",
        "    def sample_option(self, q_options: torch.Tensor, beta_options: torch.Tensor):\n",
        "        # Реализуйте выбор опции\n",
        "        ####### Здесь ваш код ########\n",
        "        if self.option is not None:\n",
        "             option_termination = bool(Bernoulli(beta_options[self.option]).sample().item())\n",
        "             if not option_termination:\n",
        "                return self.option\n",
        "        \n",
        "        option = Categorical(logits=q_options).sample().item()\n",
        "        \n",
        "        ##############################\n",
        "        return option\n",
        "    \n",
        "    def append_to_replay_buffer(self, s, a, o, r, next_s, done):\n",
        "        self.replay_buffer.append(s, a, o, r, next_s, done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "eaf69330",
      "metadata": {
        "tags": [],
        "id": "eaf69330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962315c1-c187-4dbb-9150-8c3c3cb018ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 000119, mean reward: 11.90\n",
            "step: 000245, mean reward: 12.60\n",
            "step: 000368, mean reward: 12.30\n",
            "step: 000688, mean reward: 32.00\n",
            "step: 002380, mean reward: 169.20\n",
            "step: 006096, mean reward: 371.60\n",
            "Accepted!\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.wrappers.time_limit import TimeLimit\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "\n",
        "agent = run(\n",
        "    env = TimeLimit(gym.make(env_name), 1000),\n",
        "    max_episodes = 1000,  # количество эпизодов обучения\n",
        "    hidden_size = 64,  # кол-во переменных в скрытых слоях\n",
        "    n_options = 4,\n",
        "    update_schedule = 10,\n",
        "    batch_size = 200, \n",
        "    softmax_temp = 0.01,  # softmax temperature\n",
        "    ac_lr = 0.001, # actor learning rate\n",
        "    cr_lr = 0.0002, # critic learning rate\n",
        "    termination_regularizer = 0.01, # punish early termination\n",
        "    entropy_weight = 0.5,  # punish intra-option policy over-determination\n",
        "    gamma = 0.995,  # дисконтирующий множитель,\n",
        "    replay_buffer_size = 5000,\n",
        "    critic_batch_size = 64,\n",
        "    critic_updates_per_actor = 32,\n",
        "    seed = 1337,\n",
        "    print_schedule = 10,\n",
        "    success_return = 200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"LunarLander-v2\"\n",
        "\n",
        "agent = run(\n",
        "    env = TimeLimit(gym.make(env_name), 1000),\n",
        "    max_episodes = 1000,  # количество эпизодов обучения\n",
        "    hidden_size = 128,  # кол-во переменных в скрытых слоях\n",
        "    n_options = 8,\n",
        "    update_schedule = 10,\n",
        "    batch_size = 200, \n",
        "    softmax_temp = 1.00,  # softmax temperature\n",
        "    ac_lr = 0.001, # actor learning rate\n",
        "    cr_lr = 0.0002, # critic learning rate\n",
        "    termination_regularizer = 0.1, # punish early termination\n",
        "    entropy_weight = 0.5,  # punish intra-option policy over-determination\n",
        "    gamma = 0.99,  # дисконтирующий множитель,\n",
        "    replay_buffer_size = 50000,\n",
        "    critic_batch_size = 64,\n",
        "    critic_updates_per_actor = 64,\n",
        "    seed = 1337,\n",
        "    print_schedule = 10,\n",
        "    success_return = 200\n",
        ")"
      ],
      "metadata": {
        "id": "CCXupDrfQCOG"
      },
      "id": "CCXupDrfQCOG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69516e2",
      "metadata": {
        "tags": [],
        "id": "c69516e2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def print_options(env, agent: OptionCriticAgent, option, seed=42):\n",
        "    @torch.no_grad()\n",
        "    def calc_sample(sample):\n",
        "        prediction = agent.network(sample)\n",
        "\n",
        "        q = prediction['q']\n",
        "        beta = prediction['beta']\n",
        "        pi = to_np(prediction['pi'])\n",
        "        log_pi = to_np(prediction['log_pi'])\n",
        "        \n",
        "        option = agent.sample_option(q_options=q, beta_options=beta)\n",
        "        return q, beta, pi, log_pi, option\n",
        "    \n",
        "    rng = np.random.default_rng()\n",
        "\n",
        "    states_dim = env.observation_space.shape[0]\n",
        "    xs_0 = np.linspace(-2.4, 2.4, num=50)\n",
        "    xs_1 = np.linspace(-2.4, 2.4, num=50)\n",
        "    xs_2 = np.linspace(-2.1, 2.1, num=4)\n",
        "    xs_3 = np.linspace(-2.4, 2.4, num=4)\n",
        "    \n",
        "    sample = [\n",
        "        rng.choice(xs)\n",
        "        for xs in [xs_0, xs_1, xs_2, xs_3]\n",
        "    ]\n",
        "    \n",
        "    n_plots = xs_2.size * xs_3.size\n",
        "    cols = 4\n",
        "    rows = (n_plots - 1) // cols + 1\n",
        "    \n",
        "    plt.figure(figsize=(20, 10))\n",
        "    i_plot = 1\n",
        "    for x2 in xs_2:\n",
        "        for x3 in xs_3:\n",
        "            beta_map = np.zeros((xs_0.size, xs_1.size))\n",
        "            q_map = np.zeros_like(beta_map)            \n",
        "            \n",
        "            for i, x0 in enumerate(xs_0):\n",
        "                for j, x1 in enumerate(xs_1):\n",
        "                    obs = np.array([x0, x1, x2, x3])\n",
        "                    q, beta, pi, log_pi, option = calc_sample(obs)\n",
        "                    if option > 0:\n",
        "                        beta_map[i, j] = 0\n",
        "                        q_map[i, j] = 0\n",
        "                    else:\n",
        "                        beta_map[i, j] = beta[option]\n",
        "                        q_map[i, j] = q[option] / 200\n",
        "            \n",
        "            plt.subplot(rows, 2 * cols, i_plot)\n",
        "            plt.imshow(q_map, vmin=0, vmax=1)\n",
        "            plt.subplot(rows, 2 * cols, i_plot + 1)\n",
        "            plt.imshow(beta_map, vmin=0, vmax=1)\n",
        "            i_plot += 2\n",
        "    plt.show()\n",
        "\n",
        "for i in range(agent.n_options):\n",
        "    print_options(\n",
        "        env=gym.make(env_name),\n",
        "        agent=agent,\n",
        "        option=i\n",
        "    )\n",
        "    print('===========')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e2b29aa6",
      "metadata": {
        "id": "e2b29aa6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}