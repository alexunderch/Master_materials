{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7sfWBeBq8Wx"
      },
      "source": [
        "## DQN\n",
        "\n",
        "В данном пункте мы будем использовать библиотеку pytorch для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hhWvNOLaGhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcb525a-57be-4f94-8e49-a0e82618b5e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip install \"gymnasium[classic-control, atari, accept-rom-license]\" --quiet\n",
        "    !pip install piglet --quiet\n",
        "    !pip install imageio_ffmpeg --quiet\n",
        "    !pip install moviepy==1.0.3 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SRxZWc0aGhN"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0_I5PeZaGhO"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRnOxiAZrOFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd25c2ba-8e27-4e25-d9ac-e048e0bd35b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.03020493,  0.02116681,  0.03312651, -0.02458797], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYbIV7w42Fp1"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
        "Для начала попробуйте использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9zj305oaGhP"
      },
      "source": [
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "$$\n",
        "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')] \\\\\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "где\n",
        "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние \n",
        "* $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BFkc4eN16Lh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jztg7VraGhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf503ea-3e18-4a39-9847-ac8ce4fb4422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action_space: 2 \n",
            "State_space: (4,)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "print(f'Action_space: {n_actions} \\nState_space: {env.observation_space.shape}')\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOxH_6wxaGhQ"
      },
      "source": [
        "Задавайте небольшой размер скрытых слоев, например не больше 200.\n",
        "Определяем граф вычислений:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee7xFTYsaGhS"
      },
      "outputs": [],
      "source": [
        "def create_network(input_dim, hidden_dims, output_dim):\n",
        "    # network = nn.Sequential(\n",
        "    #    torch.nn.Linear(state_dim[0], ...),\n",
        "    #    torch.nn.ReLU(),\n",
        "    #    ...\n",
        "    # )\n",
        "    ####### Здесь ваш код ########\n",
        "    hidden_dims = [input_dim] + hidden_dims + [output_dim]\n",
        "    layers = []\n",
        "    for w1, w2 in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
        "        layers.append(nn.Tanh())\n",
        "        layers.append(nn.Linear(w1, w2))\n",
        "    layers = layers[1:] \n",
        "    network = nn.Sequential(*layers)\n",
        "    ##############################\n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_network(10, [128, 128], 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EIzU6eaeyv8",
        "outputId": "d19e571b-63a5-46bf-add6-f6e3a8775128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=10, out_features=128, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (3): Tanh()\n",
              "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxIIk-MdaGhS"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(network, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = network(state).detach().numpy()\n",
        "    \n",
        "    # action = \n",
        "    ####### Здесь ваш код ########\n",
        "    if np.random.binomial(1, epsilon):\n",
        "        action = np.random.choice(np.arange(n_actions))\n",
        "    else:\n",
        "        action = np.argmax(Q_s)\n",
        "    ##############################\n",
        "    \n",
        "    action = int(action)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI80Z3UKaGhT"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(\n",
        "        network, states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False, regularizer=.1\n",
        "):\n",
        "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
        "    \n",
        "    # переводим входные данные в тензоры\n",
        "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
        "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
        "    \n",
        "    \n",
        "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32) # shape: [batch_size, state_size]\n",
        "    is_done = torch.tensor(is_done, dtype=torch.bool)    # shape: [batch_size]\n",
        "\n",
        "    # получаем значения q для всех действий из текущих состояний\n",
        "    predicted_qvalues = network(states)\n",
        "\n",
        "    # получаем q-values для выбранных действий\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "    # применяем сеть для получения q-value для следующих состояний (next_states)\n",
        "    # predicted_next_qvalues =\n",
        "    ####### Здесь ваш код ########\n",
        "    predicted_next_qvalues = network(next_states).detach()\n",
        "    ##############################\n",
        "    \n",
        "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
        "    # next_state_values =\n",
        "    ####### Здесь ваш код ########\n",
        "    next_state_values = torch.max(predicted_next_qvalues, dim = -1)[0]  \n",
        "    ##############################\n",
        "    \n",
        "    assert next_state_values.dtype == torch.float32\n",
        "\n",
        "    # вычисляем target q-values для функции потерь\n",
        "    #  target_qvalues_for_actions =\n",
        "    ####### Здесь ваш код ########\n",
        "    target_qvalues_for_actions = rewards + gamma * next_state_values \n",
        "    ##############################\n",
        "    \n",
        "    # для последнего действия в эпизоде используем \n",
        "    # упрощенную формулу Q(s,a) = r(s,a), \n",
        "    # т.к. s' для него не существует\n",
        "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "    \n",
        "    losses = (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
        "\n",
        "    # MSE loss для минимизации\n",
        "    loss = torch.mean(losses)\n",
        "    # добавляем регуляризацию на значения Q \n",
        "    loss += regularizer * predicted_qvalues_for_actions.mean()\n",
        "    \n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim(\n",
        "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert next_state_values.data.dim(\n",
        "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target_qvalues_for_actions.data.dim(\n",
        "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmWwgkHaGhU"
      },
      "source": [
        "## Simple DQN\n",
        "\n",
        "Немного модифицированная версия кода, запускающего обучение Q-learning из прошлой тетрадки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3Ps-K5uaGhU"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, network, opt, t_max=300, epsilon=0, train=False):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "        \n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "            loss, _ = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rRkiP_xaGhV"
      },
      "outputs": [],
      "source": [
        "def test_dqn():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _ = generate_session(env, network, opt, epsilon=eps, train=True)\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew = generate_session(env, network, opt, epsilon=eps, train=False)\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47wFETCHaGhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16243b04-677d-4032-b127-bd297660bac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: #49\tmean reward = 9.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.000\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 10.000\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 13.000\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 13.600\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 14.800\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 32.800\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 32.800\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 33.200\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 43.200\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 45.000\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 34.000\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 40.800\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 45.400\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 38.800\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 40.800\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 38.600\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 32.400\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 27.400\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 48.600\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 45.000\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 98.400\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 154.400\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 192.200\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 167.400\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 179.800\tepsilon = 0.037\n",
            "Epoch: #1349\tmean reward = 173.600\tepsilon = 0.034\n",
            "Epoch: #1399\tmean reward = 133.600\tepsilon = 0.030\n",
            "Epoch: #1449\tmean reward = 139.200\tepsilon = 0.027\n",
            "Epoch: #1499\tmean reward = 146.000\tepsilon = 0.025\n",
            "Epoch: #1549\tmean reward = 136.000\tepsilon = 0.022\n",
            "Epoch: #1599\tmean reward = 142.200\tepsilon = 0.020\n",
            "Epoch: #1649\tmean reward = 141.400\tepsilon = 0.018\n",
            "Epoch: #1699\tmean reward = 153.600\tepsilon = 0.017\n",
            "Epoch: #1749\tmean reward = 183.600\tepsilon = 0.015\n",
            "Epoch: #1799\tmean reward = 238.000\tepsilon = 0.014\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsOShRP2aGhW"
      },
      "source": [
        "## DQN with Experience Replay\n",
        "\n",
        "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', done)\\}$.\n",
        "\n",
        "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rUrkRLeaGhW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    ####### Здесь ваш код ########\n",
        "    idxes = [random.choice(range(len(replay_buffer))) for _ in range(n_samples)]\n",
        "    # collect <s,a,r,s',done> for each index\n",
        "    \n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    next_states = []\n",
        "    dones = []\n",
        "\n",
        "    for idx in idxes:\n",
        "        states.append(replay_buffer[idx][0])\n",
        "        actions.append(replay_buffer[idx][1])\n",
        "        rewards.append(replay_buffer[idx][2])\n",
        "        next_states.append(replay_buffer[idx][3])\n",
        "        dones.append(replay_buffer[idx][4])\n",
        "    ##############################\n",
        "        \n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXMcJoL2aGhW"
      },
      "outputs": [],
      "source": [
        "def generate_session_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "        \n",
        "        if train:\n",
        "            # put new sample into replay_buffer\n",
        "            ####### Здесь ваш код ########\n",
        "            replay_buffer.append((s, a, r, next_s, terminated and not truncated))\n",
        "            ##############################\n",
        "            \n",
        "            if replay_buffer and glob_step and glob_step % train_schedule == 0:\n",
        "                # sample new batch: train_batch = ...\n",
        "                ####### Здесь ваш код ########\n",
        "                train_batch = sample_batch(replay_buffer, batch_size)\n",
        "                ##############################\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "                \n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMZ3xG9haGhW"
      },
      "source": [
        "После проверки скорости обучения можете поэкспериментировать с различными `train_schedule`, `batch_size`, а также с размером буфера `replay_buffer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iIs44qGaGhX"
      },
      "outputs": [],
      "source": [
        "def test_dqn_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .8, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    train_schedule, batch_size = 5, 100\n",
        "    replay_buffer = deque(maxlen=1000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6AujAWkaGhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135f235c-2096-464e-b88a-8852be009ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: #49\tmean reward = 9.000\tepsilon = 0.725\n",
            "Epoch: #99\tmean reward = 9.000\tepsilon = 0.656\n",
            "Epoch: #149\tmean reward = 9.667\tepsilon = 0.594\n",
            "Epoch: #199\tmean reward = 13.000\tepsilon = 0.537\n",
            "Epoch: #249\tmean reward = 12.600\tepsilon = 0.486\n",
            "Epoch: #299\tmean reward = 14.200\tepsilon = 0.440\n",
            "Epoch: #349\tmean reward = 17.200\tepsilon = 0.398\n",
            "Epoch: #399\tmean reward = 25.600\tepsilon = 0.360\n",
            "Epoch: #449\tmean reward = 24.800\tepsilon = 0.326\n",
            "Epoch: #499\tmean reward = 44.600\tepsilon = 0.295\n",
            "Epoch: #549\tmean reward = 45.800\tepsilon = 0.267\n",
            "Epoch: #599\tmean reward = 101.000\tepsilon = 0.241\n",
            "Epoch: #649\tmean reward = 94.000\tepsilon = 0.218\n",
            "Epoch: #699\tmean reward = 128.400\tepsilon = 0.197\n",
            "Epoch: #749\tmean reward = 166.400\tepsilon = 0.179\n",
            "Epoch: #799\tmean reward = 193.000\tepsilon = 0.162\n",
            "Epoch: #849\tmean reward = 163.600\tepsilon = 0.146\n",
            "Epoch: #899\tmean reward = 198.200\tepsilon = 0.132\n",
            "Epoch: #949\tmean reward = 199.000\tepsilon = 0.120\n",
            "Epoch: #999\tmean reward = 188.600\tepsilon = 0.108\n",
            "Epoch: #1049\tmean reward = 217.400\tepsilon = 0.098\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn_replay_buffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy56uumTaGhX"
      },
      "source": [
        "## DQN with Prioritized Experience Replay\n",
        "\n",
        "Добавим каждому переходу, хранящемуся в памяти, значение приоритета. Популярным вариантом является абсолютное значение TD-ошибки.\n",
        "\n",
        "Однако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз - накладно. Приходится искать баланс между точностью и скоростью.\n",
        "\n",
        "Здесь мы будем делать следующее:\n",
        "\n",
        "- использовать TD-ошибку в кач-ве приоритета\n",
        "- после использования батча при обучении, обновляем значения приоритета для этого батча в памяти\n",
        "- будем периодически сортировать память для того, чтобы новые переходы заменяли собой те переходы, у которых наименьшие значения ошибки (т.е. наименьший приоритет)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyZ0xEQAaGhX"
      },
      "outputs": [],
      "source": [
        "def softmax(xs, temp=1000.):\n",
        "    if not isinstance(xs, np.ndarray):\n",
        "        xs = np.array(xs)\n",
        "    \n",
        "    # Обрати внимание, насколько большая температура по умолчанию!\n",
        "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "def sample_prioritized_batch(replay_buffer, n_samples):\n",
        "    # Sample randomly `n_samples` samples from replay buffer weighting by priority (sample's loss)\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    # Also, keep samples' indices (into `indices`) to return them too!\n",
        "    ####### Здесь ваш код ########\n",
        "    indices = np.random.choice(range(len(replay_buffer)), size = n_samples)\n",
        "    \n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    next_states = []\n",
        "    dones = []\n",
        "    \n",
        "    for idx in indices:\n",
        "        states.append(replay_buffer[idx][1])\n",
        "        actions.append(replay_buffer[idx][2])\n",
        "        rewards.append(replay_buffer[idx][3])\n",
        "        next_states.append(replay_buffer[idx][4])\n",
        "        dones.append(replay_buffer[idx][5])\n",
        "    ##############################\n",
        "        \n",
        "    batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "    return batch, indices\n",
        "\n",
        "def update_batch(replay_buffer, indices, batch, new_losses):\n",
        "    \"\"\"Updates batches with corresponding indices replacing their loss value.\"\"\"\n",
        "    states, actions, rewards, next_states, is_done = batch\n",
        "    \n",
        "    for i in range(len(indices)):\n",
        "        new_batch = new_losses[i], states[i], actions[i], rewards[i], next_states[i], is_done[i]\n",
        "        replay_buffer[indices[i]] = new_batch\n",
        "        \n",
        "def sort_replay_buffer(replay_buffer):\n",
        "    \"\"\"Sorts replay buffer to move samples with lesser loss to the beginning \n",
        "    ==> they will be replaced with the new samples earlier.\"\"\"\n",
        "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
        "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
        "    return new_rb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH0z6azOaGhY"
      },
      "outputs": [],
      "source": [
        "def generate_session_prioritized_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False, temp = 2.\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "        \n",
        "        if train:\n",
        "            # Compute new sample loss (it's the second returning value - `losses` - from compute_td_loss)\n",
        "            # we need `losses.numpy()[0]`\n",
        "            with torch.no_grad():\n",
        "                ####### Здесь ваш код ########\n",
        "                _, losses = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "                losses = losses.numpy()[0]\n",
        "                ##############################\n",
        "                \n",
        "            # put new sample into replay_buffer\n",
        "            ####### Здесь ваш код ########\n",
        "            replay_buffer.append((softmax(losses, temp=temp), s, a, r, next_s, terminated and not truncated))\n",
        "            ##############################\n",
        "            \n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % train_schedule == 0:\n",
        "                # sample new batch: train_batch, indices = ...\n",
        "                ####### Здесь ваш код ########\n",
        "                train_batch, indices = sample_prioritized_batch(replay_buffer, batch_size)\n",
        "                ##############################\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "                \n",
        "                opt.zero_grad()\n",
        "                loss, losses = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    # compute updated losses for the training batch and update batch in replay buffer\n",
        "                    ####### Здесь ваш код ########\n",
        "                    update_batch(replay_buffer, indices, train_batch, softmax(losses.detach().cpu().numpy(), temp))\n",
        "                    ##############################\n",
        "                \n",
        "            # periodically re-sort replay buffer to prioritize replacing with new samples those samples\n",
        "            # that have the least loss\n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % 25*train_schedule == 0:\n",
        "                replay_buffer = sort_replay_buffer(replay_buffer)\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "704aWtDEaGhY"
      },
      "outputs": [],
      "source": [
        "def test_dqn_prioritized_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    train_schedule, batch_size = 5, 100\n",
        "    replay_buffer = deque(maxlen=1000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_prioritized_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_prioritized_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfwoUo5oaGhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0e2d52-801d-42e7-cf16-898c3bf25551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: #49\tmean reward = 11.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 10.000\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 10.000\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 11.000\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 24.600\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 24.600\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 32.800\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 40.400\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 48.800\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 39.200\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 40.200\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 33.000\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 37.800\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 86.600\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 103.000\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 102.400\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 145.400\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 161.000\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 132.600\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 136.800\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 164.600\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 158.800\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 149.800\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 140.200\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 134.200\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 122.800\tepsilon = 0.037\n",
            "Epoch: #1349\tmean reward = 97.600\tepsilon = 0.034\n",
            "Epoch: #1399\tmean reward = 87.200\tepsilon = 0.030\n",
            "Epoch: #1449\tmean reward = 82.200\tepsilon = 0.027\n",
            "Epoch: #1499\tmean reward = 121.400\tepsilon = 0.025\n",
            "Epoch: #1549\tmean reward = 162.400\tepsilon = 0.022\n",
            "Epoch: #1599\tmean reward = 207.600\tepsilon = 0.020\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn_prioritized_replay_buffer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLu_o8fEBSrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}