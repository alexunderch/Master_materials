{"cells":[{"cell_type":"markdown","metadata":{"id":"CpceefzmsN45"},"source":["## Динамическое программирование"]},{"cell_type":"markdown","metadata":{"id":"uX-Ky982sN47"},"source":["Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n","    $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n","На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n","$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n","$$V_{(i+1)}(s) = \\max_a Q_i(s,a)$$\n","\n","Зададим напрямую модель MDP с картинки:\n","<img src=\"https://camo.githubusercontent.com/44c1a4d0063e3954f9eb6686a5b7b9763dadd54a/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f612f61642f4d61726b6f765f4465636973696f6e5f50726f636573732e737667\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3o7E4rMhOBbE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069057671,"user_tz":-180,"elapsed":312,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"b277da96-5130-4340-ebd3-1126efec333d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-10 22:44:15--  https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10969 (11K) [text/plain]\n","Saving to: ‘mdp.py.2’\n","\n","\rmdp.py.2              0%[                    ]       0  --.-KB/s               \rmdp.py.2            100%[===================>]  10.71K  --.-KB/s    in 0s      \n","\n","2023-02-10 22:44:16 (52.5 MB/s) - ‘mdp.py.2’ saved [10969/10969]\n","\n"]}],"source":["try:\n","    import colab\n","    COLAB = True\n","except ModuleNotFoundError:\n","    COLAB = False\n","    pass\n","\n","if COLAB:\n","    !wget https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2ebF4HaPsN48","executionInfo":{"status":"ok","timestamp":1676069057673,"user_tz":-180,"elapsed":49,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["transition_probs = {\n","  's0':{\n","    'a0': {'s0': 0.5, 's2': 0.5},\n","    'a1': {'s2': 1}\n","  },\n","  's1':{\n","    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n","    'a1': {'s1': 0.95, 's2': 0.05}\n","  },\n","  's2':{\n","    'a0': {'s0': 0.4, 's2': 0.6},\n","    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n","  }\n","}\n","rewards = {\n","  's1': {'a0': {'s0': +5}},\n","  's2': {'a1': {'s0': -1}}\n","}\n","\n","from mdp import MDP\n","import numpy as np\n","mdp = MDP(transition_probs, rewards, initial_state='s0')"]},{"cell_type":"markdown","metadata":{"id":"pZzoLTNHOBbH"},"source":["Теперь мы можем использовать это MDP, как и любое другое gym окружение:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jzbCLYtKOBbH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069057675,"user_tz":-180,"elapsed":51,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"f3383d6f-857d-4c76-8611-c79c77d8e24c"},"outputs":[{"output_type":"stream","name":"stdout","text":["initial state = s0\n","next_state =s2, reward = 0.0, done = False\n"]}],"source":["state = mdp.reset()\n","print('initial state =', state)\n","next_state, reward, done, info = mdp.step('a1')\n","print(f'next_state ={next_state}, reward = {reward}, done = {done}')"]},{"cell_type":"markdown","metadata":{"id":"CVvOpRYnOBbI"},"source":["Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0bcgBOZbsN5A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069057675,"user_tz":-180,"elapsed":47,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"2ee1a292-ddb4-43fb-8a28-b3ca80314223"},"outputs":[{"output_type":"stream","name":"stdout","text":["all_states = ('s0', 's1', 's2')\n","possible_actions('s1') =  ('a0', 'a1')\n","next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n","reward('s1', 'a0', 's0') =  5\n","transition_prob('s1', 'a0', 's0') =  0.7\n"]}],"source":["print(\"all_states =\", mdp.get_all_states())\n","print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n","print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n","print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n","print(\"transition_prob('s1', 'a0', 's0') = \",\n","      mdp.get_transition_prob('s1', 'a0', 's0'))"]},{"cell_type":"markdown","metadata":{"id":"WYDjsDv2sN5F"},"source":["### Задание 1\n","\n","Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n","\n","---\n","\n","`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n","\n","`2.` For $i=0, 1, 2, \\dots$\n"," \n","`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n","\n","---\n","\n","Вначале вычисляем оценку состояния-действия:\n","$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zVSz8FCRsN5G","executionInfo":{"status":"ok","timestamp":1676069057676,"user_tz":-180,"elapsed":43,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["def get_action_value(mdp, state_values, state, action, \n","                     gamma):\n","    \"\"\" Вычислеям Q(s,a) по формуле выше \"\"\"\n","    # вычислеяем оценку состояния\n","    # Q = \n","    ####### Здесь ваш код ########      \n","    Q = sum(tr_prob * (mdp.get_reward(state, action, next_state) + gamma * state_values[next_state])\n","               for next_state, tr_prob in mdp.get_next_states(state, action).items() )\n","    ##############################\n","    \n","    return Q"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"U8rRYvrpsN5I","executionInfo":{"status":"ok","timestamp":1676069057677,"user_tz":-180,"elapsed":43,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n","assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n","assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"]},{"cell_type":"markdown","metadata":{"id":"1tEz93sasN5K"},"source":["Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n","\n","$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"QA8c-2_fsN5K","executionInfo":{"status":"ok","timestamp":1676069057677,"user_tz":-180,"elapsed":43,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["def get_new_state_value(mdp, state_values, state, gamma):\n","    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n","    if mdp.is_terminal(state): \n","        return 0\n","    # V = \n","    ####### Здесь ваш код ######## \n","    V = max(get_action_value(mdp, state_values, state, action, gamma)\n","            for action in mdp.get_possible_actions(state))\n","    ##############################\n","    \n","    return V"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ym4og12hsN5N","executionInfo":{"status":"ok","timestamp":1676069057678,"user_tz":-180,"elapsed":44,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["test_Vs_copy = dict(test_Vs)\n","assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n","assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n","assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n","   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n","assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""]},{"cell_type":"markdown","metadata":{"id":"emA2sv-QsN5P"},"source":["Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"er9KwtjBsN5Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069057679,"user_tz":-180,"elapsed":45,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"c031cf6d-a760-4ff2-9310-b77557d8457a"},"outputs":[{"output_type":"stream","name":"stdout","text":["iter    0 | diff: 3.50000 | V(start): 0.000 \n","iter    1 | diff: 0.64500 | V(start): 0.000 \n","iter    2 | diff: 0.58050 | V(start): 0.581 \n","iter    3 | diff: 0.43582 | V(start): 0.866 \n","iter    4 | diff: 0.30634 | V(start): 1.145 \n","iter    5 | diff: 0.27571 | V(start): 1.421 \n","iter    6 | diff: 0.24347 | V(start): 1.655 \n","iter    7 | diff: 0.21419 | V(start): 1.868 \n","iter    8 | diff: 0.19277 | V(start): 2.061 \n","iter    9 | diff: 0.17327 | V(start): 2.233 \n","iter   10 | diff: 0.15569 | V(start): 2.389 \n","iter   11 | diff: 0.14012 | V(start): 2.529 \n","iter   12 | diff: 0.12610 | V(start): 2.655 \n","iter   13 | diff: 0.11348 | V(start): 2.769 \n","iter   14 | diff: 0.10213 | V(start): 2.871 \n","iter   15 | diff: 0.09192 | V(start): 2.963 \n","iter   16 | diff: 0.08272 | V(start): 3.045 \n","iter   17 | diff: 0.07445 | V(start): 3.120 \n","iter   18 | diff: 0.06701 | V(start): 3.187 \n","iter   19 | diff: 0.06031 | V(start): 3.247 \n","iter   20 | diff: 0.05428 | V(start): 3.301 \n","iter   21 | diff: 0.04885 | V(start): 3.350 \n","iter   22 | diff: 0.04396 | V(start): 3.394 \n","iter   23 | diff: 0.03957 | V(start): 3.434 \n","iter   24 | diff: 0.03561 | V(start): 3.469 \n","iter   25 | diff: 0.03205 | V(start): 3.502 \n","iter   26 | diff: 0.02884 | V(start): 3.530 \n","iter   27 | diff: 0.02596 | V(start): 3.556 \n","iter   28 | diff: 0.02336 | V(start): 3.580 \n","iter   29 | diff: 0.02103 | V(start): 3.601 \n","iter   30 | diff: 0.01892 | V(start): 3.620 \n","iter   31 | diff: 0.01703 | V(start): 3.637 \n","iter   32 | diff: 0.01533 | V(start): 3.652 \n","iter   33 | diff: 0.01380 | V(start): 3.666 \n","iter   34 | diff: 0.01242 | V(start): 3.678 \n","iter   35 | diff: 0.01117 | V(start): 3.689 \n","iter   36 | diff: 0.01006 | V(start): 3.699 \n","iter   37 | diff: 0.00905 | V(start): 3.708 \n","iter   38 | diff: 0.00815 | V(start): 3.717 \n","iter   39 | diff: 0.00733 | V(start): 3.724 \n","iter   40 | diff: 0.00660 | V(start): 3.731 \n","iter   41 | diff: 0.00594 | V(start): 3.736 \n","iter   42 | diff: 0.00534 | V(start): 3.742 \n","iter   43 | diff: 0.00481 | V(start): 3.747 \n","iter   44 | diff: 0.00433 | V(start): 3.751 \n","iter   45 | diff: 0.00390 | V(start): 3.755 \n","iter   46 | diff: 0.00351 | V(start): 3.758 \n","iter   47 | diff: 0.00316 | V(start): 3.762 \n","iter   48 | diff: 0.00284 | V(start): 3.764 \n","iter   49 | diff: 0.00256 | V(start): 3.767 \n","iter   50 | diff: 0.00230 | V(start): 3.769 \n","iter   51 | diff: 0.00207 | V(start): 3.771 \n","iter   52 | diff: 0.00186 | V(start): 3.773 \n","iter   53 | diff: 0.00168 | V(start): 3.775 \n","iter   54 | diff: 0.00151 | V(start): 3.776 \n","iter   55 | diff: 0.00136 | V(start): 3.778 \n","iter   56 | diff: 0.00122 | V(start): 3.779 \n","iter   57 | diff: 0.00110 | V(start): 3.780 \n","iter   58 | diff: 0.00099 | V(start): 3.781 \n","Принято! Алгоритм сходится!\n"]}],"source":["def value_iteration(mdp, state_values=None,\n","    gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n","    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n","    # инициализируем V(s)\n","    state_values = state_values or \\\n","    {s : 0 for s in mdp.get_all_states()}\n","    \n","    for i in range(num_iter):\n","        # Вычисляем новые полезности состояний, \n","        # используя функции, определенные выше. \n","        # Должен получиться словарь {s: new_V(s)}\n","        # new_state_values = \n","        ####### Здесь ваш код ########\n","        new_state_values = {s: get_new_state_value(mdp, state_values, s, gamma) for s in mdp.get_all_states()}\n","        ##############################\n","        \n","        assert isinstance(new_state_values, dict)\n","\n","        # Считаем разницу\n","        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n","        \n","        print(\"iter %4i | diff: %6.5f | V(start): %.3f \"%\n","          (i, diff, new_state_values[mdp._initial_state]))\n","        \n","        state_values = new_state_values\n","        if diff < min_difference:\n","            print(\"Принято! Алгоритм сходится!\")\n","            break\n","            \n","    return state_values\n","\n","state_values = value_iteration(mdp,\n","        num_iter = 100, min_difference = 0.001)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3cKoN2-tsN5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069057681,"user_tz":-180,"elapsed":41,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"9695408f-27d7-4e9b-ce77-d6e519bb3708"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final state values: {'s0': 3.7810348735476405, 's1': 7.294006423867229, 's2': 4.202140275227048}\n","{'s0': 3.7810348735476405, 's1': 7.294006423867229, 's2': 4.202140275227048}\n"]}],"source":["print(\"Final state values:\", state_values)\n","print(state_values)\n","assert abs(state_values['s0'] - 3.781) < 0.01\n","assert abs(state_values['s1'] - 7.294) < 0.01\n","assert abs(state_values['s2'] - 4.202) < 0.01"]},{"cell_type":"markdown","metadata":{"id":"bSIBT-49sN5X"},"source":["По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n","$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"x9m7NArGsN5Y","executionInfo":{"status":"ok","timestamp":1676069057683,"user_tz":-180,"elapsed":33,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["def get_optimal_action(mdp, state_values, state, \n","                       gamma=0.9):\n","    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n","    if mdp.is_terminal(state): return None\n","    \n","    actions = mdp.get_possible_actions(state)\n","    # выбираем лучшее действие\n","    # i = \n","    ####### Здесь ваш код ########\n","    q = {\n","        action: get_action_value(mdp, state_values, state, action, gamma)\n","        for action in actions\n","    }\n","    action = max(q, key=q.get)\n","    ##############################\n","    \n","    return action"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"tCt9IjLnsN5Z","executionInfo":{"status":"ok","timestamp":1676069057687,"user_tz":-180,"elapsed":37,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n","assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n","assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n","\n","assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n","    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n","assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n","    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""]},{"cell_type":"code","execution_count":13,"metadata":{"id":"8fRn3lB_OBbO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069057926,"user_tz":-180,"elapsed":275,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"5a12f4ea-1db7-4cf0-81c0-3d0268678139"},"outputs":[{"output_type":"stream","name":"stdout","text":["average reward:  0.4853\n"]}],"source":["# Проверим среднее вознаграждение агента\n","\n","s = mdp.reset()\n","rewards = []\n","for _ in range(10000):\n","    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n","    rewards.append(r)\n","\n","print(\"average reward: \", np.mean(rewards))\n","\n","assert(0.40 < np.mean(rewards) < 0.55)"]},{"cell_type":"markdown","metadata":{"id":"-omPxnyksN5b"},"source":["### Задание 2\n","\n","Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"QhBAOCyEsN5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069057927,"user_tz":-180,"elapsed":22,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"77329191-1c8f-4649-a137-457708838fc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["*FFF\n","FHFH\n","FFFH\n","HFFG\n","\n","iter    0 | diff: 1.00000 | V(start): 0.000 \n","iter    1 | diff: 0.90000 | V(start): 0.000 \n","iter    2 | diff: 0.81000 | V(start): 0.000 \n","iter    3 | diff: 0.72900 | V(start): 0.000 \n","iter    4 | diff: 0.65610 | V(start): 0.000 \n","iter    5 | diff: 0.59049 | V(start): 0.590 \n","iter    6 | diff: 0.00000 | V(start): 0.590 \n","Принято! Алгоритм сходится!\n"]}],"source":["from mdp import FrozenLakeEnv\n","mdp = FrozenLakeEnv(slip_chance=0)\n","\n","mdp.render()\n","state_values = value_iteration(mdp)"]},{"cell_type":"markdown","metadata":{"id":"-kS3QDqNsN5h"},"source":["Визуализируем нашу стратегию."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"wRbL3izdsN5h","executionInfo":{"status":"ok","timestamp":1676069058204,"user_tz":-180,"elapsed":296,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["def draw_policy(mdp, state_values, gamma=0.9):\n","    \"\"\"функция визуализации стратегии\"\"\"\n","    plt.figure(figsize=(3, 3))\n","    h, w = mdp.desc.shape\n","    states = sorted(mdp.get_all_states())\n","    V = np.array([state_values[s] for s in states])\n","    Pi = {\n","        s: get_optimal_action(mdp, state_values, s, gamma)\n","        for s in states}\n","    plt.imshow(V.reshape(w, h),\n","               cmap='gray', interpolation='none',\n","               clim=(0, 1))\n","    ax = plt.gca()\n","    ax.set_xticks(np.arange(h) - .5)\n","    ax.set_yticks(np.arange(w) - .5)\n","    ax.set_xticklabels([])\n","    ax.set_yticklabels([])\n","    Y, X = np.mgrid[0:4, 0:4]\n","    a2uv = {'left': (-1, 0), 'down': (0, -1),\n","            'right': (1, 0), 'up': (-1, 0)}\n","    for y in range(h):\n","        for x in range(w):\n","            plt.text(x, y, str(mdp.desc[y, x].item()),\n","                     color='g', size=12,\n","                     verticalalignment='center',\n","                     horizontalalignment='center',\n","                     fontweight='bold')\n","            a = Pi[y, x]\n","            if a is None: continue\n","            u, v = a2uv[a]\n","            plt.arrow(x, y, u * .3, -v * .3,\n","                      color='m', head_width=0.1,\n","                      head_length=0.1)\n","    plt.grid(color='b', lw=2, ls='-')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"lQiqx1QesN5k","colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"status":"ok","timestamp":1676069088640,"user_tz":-180,"elapsed":30440,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"79381875-bb0a-4953-e685-b3380eb2dbb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["after iteration 29\n","iter    0 | diff: 0.00000 | V(start): 0.198 \n","Принято! Алгоритм сходится!\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 216x216 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["from IPython.display import clear_output\n","from time import sleep\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n","state_values = {s : 0 for s in mdp.get_all_states()}\n","\n","for i in range(30):\n","    clear_output(True)\n","    print(\"after iteration %i\"%i)\n","    state_values = value_iteration(mdp, \n","                            state_values, num_iter=1)\n","    draw_policy(mdp, state_values)\n","    sleep(0.5)"]},{"cell_type":"markdown","metadata":{"id":"C0U-3EN-OBbS"},"source":["Посмотрим на оптимальную стратегию:"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"3YYd8z6IOBbS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069088641,"user_tz":-180,"elapsed":11,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"f76fab93-3ab9-4e2c-8103-3b9e3ba94eb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["*FFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","S*FFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SF*FFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFF*FFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFF*FFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFF*FFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFFF*FF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFFFFFF\n","FFFFF*FF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHF*FF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFF*F\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHF*FF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFF*F\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHF*FF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","right\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFF*F\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFFF*\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHF*\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFF*\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFH*\n","FHFFHFHF\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFH*\n","FFFHFFFG\n","\n","down\n","\n","SFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFF*\n","\n"]}],"source":["s = mdp.reset()\n","mdp.render()\n","for t in range(100):\n","    a = get_optimal_action(mdp, state_values, s, 0.9)\n","    print(a, end='\\n\\n')\n","    s, r, done, _ = mdp.step(a)\n","    mdp.render()\n","    if done:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"62xktmhWsN5n"},"source":["Тестируем на более сложном варианте окружения:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"BZsHbVNzsN5o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069089355,"user_tz":-180,"elapsed":723,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"dc64ace0-223d-4168-8d37-7619e171a767"},"outputs":[{"output_type":"stream","name":"stdout","text":["iter    0 | diff: 0.80000 | V(start): 0.000 \n","iter    1 | diff: 0.57600 | V(start): 0.000 \n","iter    2 | diff: 0.41472 | V(start): 0.000 \n","iter    3 | diff: 0.29860 | V(start): 0.000 \n","iter    4 | diff: 0.24186 | V(start): 0.000 \n","iter    5 | diff: 0.19349 | V(start): 0.000 \n","iter    6 | diff: 0.15325 | V(start): 0.000 \n","iter    7 | diff: 0.12288 | V(start): 0.000 \n","iter    8 | diff: 0.09930 | V(start): 0.000 \n","iter    9 | diff: 0.08037 | V(start): 0.000 \n","iter   10 | diff: 0.06426 | V(start): 0.000 \n","iter   11 | diff: 0.05129 | V(start): 0.000 \n","iter   12 | diff: 0.04330 | V(start): 0.000 \n","iter   13 | diff: 0.03802 | V(start): 0.033 \n","iter   14 | diff: 0.03332 | V(start): 0.058 \n","iter   15 | diff: 0.02910 | V(start): 0.087 \n","iter   16 | diff: 0.01855 | V(start): 0.106 \n","iter   17 | diff: 0.01403 | V(start): 0.120 \n","iter   18 | diff: 0.00810 | V(start): 0.128 \n","iter   19 | diff: 0.00555 | V(start): 0.133 \n","iter   20 | diff: 0.00321 | V(start): 0.137 \n","iter   21 | diff: 0.00247 | V(start): 0.138 \n","iter   22 | diff: 0.00147 | V(start): 0.139 \n","iter   23 | diff: 0.00104 | V(start): 0.140 \n","iter   24 | diff: 0.00058 | V(start): 0.140 \n","iter   25 | diff: 0.00036 | V(start): 0.141 \n","iter   26 | diff: 0.00024 | V(start): 0.141 \n","iter   27 | diff: 0.00018 | V(start): 0.141 \n","iter   28 | diff: 0.00012 | V(start): 0.141 \n","iter   29 | diff: 0.00007 | V(start): 0.141 \n","iter   30 | diff: 0.00004 | V(start): 0.141 \n","iter   31 | diff: 0.00003 | V(start): 0.141 \n","iter   32 | diff: 0.00001 | V(start): 0.141 \n","iter   33 | diff: 0.00001 | V(start): 0.141 \n","Принято! Алгоритм сходится!\n","Cреднее вознаграждение: 0.718\n","Принято!\n"]}],"source":["mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n","state_values = value_iteration(mdp)\n","\n","total_rewards = []\n","for game_i in range(1000):\n","    s = mdp.reset()\n","    rewards = []\n","    for t in range(100):\n","        # выполняем оптимальное действие в окружении\n"," \n","        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n","       \n","        rewards.append(r)\n","        if done: break\n","    total_rewards.append(np.sum(rewards))\n","    \n","print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n","assert(0.6 <= np.mean(total_rewards) <= 0.8)\n","print(\"Принято!\")"]},{"cell_type":"markdown","metadata":{"id":"xdMVQ1XQsN5s"},"source":["### Задание 3\n","\n","Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n","\n","---\n","Initialize $\\pi_0$   `// случайно`\n","\n","For $n=0, 1, 2, \\dots$\n","- Считаем функцию $V^{\\pi_{n}}$\n","- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n","- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n","---\n","\n","PI включает в себя оценку полезности состояния, как внутренний шаг."]},{"cell_type":"markdown","metadata":{"id":"7Pg53dh5sN5s"},"source":["Вначале оценим полезности, используя текущую стратегию:\n","$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n","    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve.\n","\n","$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s') R(s,\\pi(s),s') + \\gamma P(s,\\pi(s),s') V^{\\pi}(s')]$$\n","\n","$$V^\\pi(s) - \\sum_{s'} \\gamma P(s,\\pi(s),s')V^{\\pi}(s') = \\sum_{s'} P(s,\\pi(s),s') R(s,\\pi(s),s') $$"]},{"cell_type":"code","execution_count":92,"metadata":{"id":"sP2gL08osN5u","executionInfo":{"status":"ok","timestamp":1676069708495,"user_tz":-180,"elapsed":3,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["from numpy.linalg import solve\n","\n","def compute_vpi(mdp, policy, gamma):\n","    \"\"\"\n","    Считем V^pi(s) для всех состояний, согласно стратегии.\n","    :param policy: словарь состояние->действие {s : a}\n","    :returns: словарь {state : V^pi(state)}\n","    \"\"\"\n","    states = mdp.get_all_states()\n","    A, b = [], []\n","    for i, state in enumerate(states):\n","        if state in policy:\n","            action = policy[state]\n","            # формируем матрицу A (... A.append(...))\n","            ####### Здесь ваш код ########\n","            A.append(\n","                [int(i == j) - gamma * mdp.get_transition_prob(state, action, next_state)\n","                for j, next_state in enumerate(states)]\n","            )\n","         \n","            ##############################\n","            \n","            # и вектор b (b.append(...))\n","            ####### Здесь ваш код ########\n","\n","            b.append(\n","                sum(tr_prob * mdp.get_reward(state, action, next_state)\n","                for next_state, tr_prob in mdp.get_next_states(state, action).items())\n","                )\n","            ##############################\n","            \n","        else:\n","            # формируем матрицу A (... A.append(...))\n","            ####### Здесь ваш код ########\n","            A.append(\n","                [int(i == j) for j in range(len(states))])\n","            ##############################\n","            \n","            # вектор b (b.append(...))\n","            ####### Здесь ваш код ########\n","            b.append(0)\n","            ##############################\n","    A = np.array(A)\n","    b = np.array(b)\n","    \n","    values = solve(A, b)\n","    \n","    state_values = {states[i] : values[i] \n","                    for i in range(len(states))}\n","    return state_values"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"1aG9dcJVsN5w","executionInfo":{"status":"ok","timestamp":1676069708911,"user_tz":-180,"elapsed":7,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["transition_probs = {\n","    's0': {\n","        'a0': {'s0': 0.5, 's2': 0.5},\n","        'a1': {'s2': 1}\n","    },\n","    's1': {\n","        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n","        'a1': {'s1': 0.95, 's2': 0.05}\n","    },\n","    's2': {\n","        'a0': {'s0': 0.4, 's1': 0.6},\n","        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n","    }\n","}\n","rewards = {\n","    's1': {'a0': {'s0': +5}},\n","    's2': {'a1': {'s0': -1}}\n","}\n","mdp = MDP(transition_probs, rewards, initial_state='s0')\n","state = mdp.reset()\n","gamma = 0.9\n","\n","test_policy = {\n","    s: np.random.choice(mdp.get_possible_actions(s))\n","    for s in mdp.get_all_states()}\n","new_vpi = compute_vpi(mdp, test_policy, gamma)\n","\n","assert type(new_vpi) is dict, \\\n","    \"функция compute_vpi должна возвращать словарь \\\n","    {состояние s : V^pi(s) }\""]},{"cell_type":"markdown","metadata":{"id":"hD-bBuwKsN5y"},"source":["Теперь обновляем стратегию на основе новых значений полезностей:"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"5pKbE1visN50","executionInfo":{"status":"ok","timestamp":1676069711288,"user_tz":-180,"elapsed":2,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["def compute_new_policy(mdp, vpi, gamma):\n","    \"\"\"\n","    Рассчитываем новую стратегию\n","    :param vpi: словарь {state : V^pi(state) }\n","    :returns: словарь {state : оптимальное действие}\n","    \"\"\"\n","    Q = {}\n","    for state in mdp.get_all_states():\n","        Q[state] = {}\n","        for action in mdp.get_possible_actions(state):\n","            values = []\n","            for next_state in mdp.get_next_states(state,\n","                                                  action):\n","                r = mdp.get_reward(state, action, next_state)\n","                p = mdp.get_transition_prob(state, action,\n","                                            next_state)\n","                # values.append(...)\n","                ####### Здесь ваш код ########\n","                values.append(p * (r + gamma * vpi[next_state]))\n","                ##############################\n","                \n","            Q[state][action] = sum(values)\n","    policy = {}\n","    for state in mdp.get_all_states():\n","        actions = mdp.get_possible_actions(state)\n","        if actions:\n","            # выбираем оптимальное действие в state\n","            # policy[state] = ... \n","            ####### Здесь ваш код ########\n","            policy[state] = actions[np.argmax([Q[state][action] for action in actions])]\n","            ##############################\n","            \n","    return policy"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"SKkZnfGRsN53","executionInfo":{"status":"ok","timestamp":1676069744151,"user_tz":-180,"elapsed":294,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["new_policy = compute_new_policy(mdp, new_vpi, gamma)\n","assert type(new_policy) is dict, \\\n","\"функция compute_new_policy должна возвращать словарь \\\n","{состояние s: оптимальное действие}\""]},{"cell_type":"markdown","metadata":{"id":"XHax1qV5sN55"},"source":["Собираем все в единый цикл:"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"o4rfeoFasN56","executionInfo":{"status":"ok","timestamp":1676069744603,"user_tz":-180,"elapsed":3,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"outputs":[],"source":["def policy_iteration(mdp, policy=None, gamma = 0.9, \n","                 num_iter = 1000, min_difference = 1e-5):\n","    \"\"\" \n","    Запускаем цикл итерации по стратегиям \n","    Если стратегия не определена, задаем случайную\n","    \"\"\"\n","    for i in range(num_iter):\n","        if not policy:\n","            policy = {}\n","            for s in mdp.get_all_states():\n","                if mdp.get_possible_actions(s):\n","                    policy[s] = np.random \\\n","                    .choice(mdp.get_possible_actions(s))\n","        # state_values = \n","        ####### Здесь ваш код ########\n","        state_values = compute_vpi(mdp, policy, gamma)\n","        ##############################\n","        \n","        #policy =\n","        ####### Здесь ваш код ########\n","        policy = compute_new_policy(mdp, state_values, gamma)\n","        ##############################        \n","    return state_values, policy"]},{"cell_type":"markdown","metadata":{"id":"NJm34Gc5sN57"},"source":["Тестируем на FrozenLake."]},{"cell_type":"code","execution_count":99,"metadata":{"id":"NC7hXWhssN58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676069763475,"user_tz":-180,"elapsed":1197,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}},"outputId":"2e9d832e-e748-4efb-90f9-73977b252655"},"outputs":[{"output_type":"stream","name":"stdout","text":["average reward:  0.893\n","Принято!\n"]}],"source":["mdp = FrozenLakeEnv(slip_chance=0.1)\n","state_values, policy = policy_iteration(mdp)\n","\n","total_rewards = []\n","for game_i in range(1000):\n","    s = mdp.reset()\n","    rewards = []\n","    for t in range(100):\n","        s, r, done, _ = mdp.step(policy[s])\n","        rewards.append(r)\n","        if done: break\n","    total_rewards.append(np.sum(rewards))\n","    \n","print(\"average reward: \", np.mean(total_rewards))\n","assert(0.8 <= np.mean(total_rewards) <= 0.95)\n","print(\"Принято!\")"]},{"cell_type":"code","source":[],"metadata":{"id":"n09MkbfVbUAA","executionInfo":{"status":"aborted","timestamp":1676069089359,"user_tz":-180,"elapsed":11,"user":{"displayName":"Alexander Chernyavskiy","userId":"12341725006329704124"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1L7CFxfj3omoSy-yuL0QZVhtXoSNjrPvK","timestamp":1675946898230}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}