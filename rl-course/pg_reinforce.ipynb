{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYbwedxDQx8o"
      },
      "source": [
        "## Градиент стратегии: REINFORCE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV4Wi1LQQx8q"
      },
      "source": [
        "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [\\nabla_\\theta \\ln \\pi_\\theta(a \\vert s) Q^\\pi(s, a)]$$\n",
        "\n",
        "Если использовать метод Монте-Карло в качестве несмещенной оценки $Q^\\pi(s, a)$ отдачу $R_t$, то тогда происходит переход к алгоритму REINFORCE и обновление весов будет осуществляться по правилу:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = [R_t \\ln \\nabla_\\theta \\pi_\\theta(A_t \\vert S_t)]$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip install \"gymnasium[classic-control, atari, accept-rom-license]\" --quiet\n",
        "    !pip install piglet --quiet\n",
        "    !pip install imageio_ffmpeg --quiet\n",
        "    !pip install moviepy==1.0.3 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0PQiDMOWyuu",
        "outputId": "94b5ce0c-6a78-4f09-e5da-ffb70a8b9bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "j5-iWob5Qx8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29036e67-13ff-4ea4-ea7d-162b6a7f6af3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IeIRpZZQx8r"
      },
      "source": [
        "### Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qAOyb-wQx8s"
      },
      "outputs": [],
      "source": [
        "def print_mean_reward(step, episode_rewards):\n",
        "    if not episode_rewards:\n",
        "        return\n",
        "\n",
        "    mean_reward = round(sum(episode_rewards) / len(episode_rewards), 2)\n",
        "    print(f\"step: {str(step).zfill(6)}, mean reward: {mean_reward}\")\n",
        "    return mean_reward\n",
        "    \n",
        "    \n",
        "class Rollout:\n",
        "    def __init__(self):\n",
        "        self.logprobs = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "        \n",
        "    def append(self, log_prob, reward, done):\n",
        "        self.logprobs.append(log_prob)\n",
        "        self.rewards.append(reward)\n",
        "        self.is_terminals.append(done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBfieMkKQx8s"
      },
      "outputs": [],
      "source": [
        "# Реализуйте класс, задающий стратегию агента.\n",
        "# Подсказки:\n",
        "#     1) можно воспользоваться базовым классом `torch.nn.Module`,\n",
        "#     2) размер нейронной сети можно выбрать таким: (input_dim, hidden_dim, output_dim),\n",
        "#     3) в качестве функции активации возьмите гиперболический тангенс или ReLU\n",
        "#     4) подумайте, как получить на выходе из нейронной сети вероятности действий,\n",
        "#     5) для выбора действия в соответствии со стратегией, можно воспользоваться `torch.distributions.Categorical`\n",
        "#     6) помните, что помимо самого действия вам позже также пригодится логарифм его вероятности\n",
        "####### Здесь ваш код ########\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_hidden_layers):\n",
        "        super().__init__()\n",
        "        modules = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
        "        for _ in range(n_hidden_layers):\n",
        "            modules.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
        "        modules.extend([nn.Linear(hidden_dim, output_dim), nn.Softmax(-1)])\n",
        "        self.net = nn.Sequential(*modules)\n",
        "    def forward(self, state_x: torch.Tensor):\n",
        "        probs = self.net(state_x)\n",
        "        distr = torch.distributions.Categorical(probs=probs)   \n",
        "        action = distr.sample()\n",
        "        log_prob = distr.log_prob(action)\n",
        "        return action.item(), log_prob[None]\n",
        "##############################\n",
        "\n",
        "\n",
        "class ReinforceAgent:\n",
        "    def __init__(self, state_dim, hidden_dim, action_dim, n_latent_var, lr, gamma):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Инициализируйте стратегию агента и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
        "        ####### Здесь ваш код ########\n",
        "        self.policy = Agent(state_dim, hidden_dim, action_dim, n_latent_var).to(device)\n",
        "        print(self.policy)\n",
        "        self.optimizer = torch.optim.Adam(params=self.policy.parameters(), lr = lr)\n",
        "        ##############################\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "    def act(self, state):\n",
        "        # Произведите выбор действия и верните кортеж (действие, логарифм вероятности этого действия)\n",
        "        ####### Здесь ваш код ########\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.tensor(state, device=device)\n",
        "        return self.policy(state)\n",
        "        ##############################\n",
        "\n",
        "    def update(self, rollout: Rollout):\n",
        "        # Конвертируйте накопленный список вознаграждений в список отдач. Назовем его `rewards`\n",
        "        # Подсказки:\n",
        "        #    1) обход списка стоит делать в обратном порядке, \n",
        "        #    2) не забывайте сбрасывать отдачу при окончании эпизода\n",
        "        # rewards = \n",
        "        ####### Здесь ваш код ########\n",
        "        rewards = rollout.rewards\n",
        "        \n",
        "        cumulative_rewards = [rewards[-1]]\n",
        "        for i in range(len(rewards)-2,-1,-1):\n",
        "            cumulative_rewards.append(rewards[i] + self.gamma * cumulative_rewards[-1] * rollout.is_terminals[i])\n",
        "           \n",
        "        cumulative_rewards = torch.tensor(cumulative_rewards[::-1], device = device)[:, None] \n",
        "\n",
        "        cumulative_rewards = (cumulative_rewards - torch.mean(cumulative_rewards)) / (torch.std(cumulative_rewards) + 1e-6)\n",
        "        ##############################\n",
        "        # Вычислите ошибку `loss` и произведите шаг обновления градиентным спуском\n",
        "        # Подсказки: используйте `.to(device)`, чтобы разместить тензор на соотв. цпу/гпу\n",
        "        ####### Здесь ваш код ########\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        loss = -torch.mean(torch.stack(rollout.logprobs).to(device) * cumulative_rewards)\n",
        "        # print(loss.item())       \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        ##############################\n",
        "\n",
        "\n",
        "def run(env: gym.Env, hidden_size: int, n_layers: int, lr: float, gamma: float, max_episodes: int, rollout_size: int):\n",
        "    ####### Здесь ваш код ########\n",
        "    ##############################\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    print(action_dim)\n",
        "\n",
        "    agent = ReinforceAgent(state_dim, hidden_size, action_dim, n_layers, lr, gamma)\n",
        "    step = 0\n",
        "    rollout = Rollout()\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i_episode in range(1, max_episodes + 1):\n",
        "        cumulative_reward = 0        \n",
        "        terminated = False\n",
        "        state, _ = env.reset()\n",
        "        \n",
        "        while not terminated:\n",
        "            step += 1\n",
        "            \n",
        "            action, log_prob = agent.act(state)\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            rollout.append(log_prob, reward, not terminated)\n",
        "            cumulative_reward += reward\n",
        "            terminated |= truncated\n",
        "\n",
        "        episode_rewards.append(cumulative_reward)\n",
        "\n",
        "        \n",
        "        # выполняем обновление\n",
        "        if len(rollout.rewards) >= rollout_size:\n",
        "            agent.update(rollout)\n",
        "            mean_reward = print_mean_reward(step, episode_rewards) \n",
        "            if mean_reward >= 200:\n",
        "                print('Принято!')\n",
        "                return\n",
        "            rollout = Rollout()\n",
        "            episode_rewards = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK7S25viQx8t"
      },
      "source": [
        "### Определяем гиперпараметры и запускаем обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5J6wprQQx8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69df1830-62e0-4f9d-a9cf-67f713d7b878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Agent(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
            "    (5): Softmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "step: 000505, mean reward: 22.95\n",
            "step: 001028, mean reward: 32.69\n",
            "step: 001556, mean reward: 58.67\n",
            "step: 002139, mean reward: 83.29\n",
            "step: 002798, mean reward: 131.8\n",
            "step: 003489, mean reward: 172.75\n",
            "step: 004096, mean reward: 121.4\n",
            "step: 004632, mean reward: 107.2\n",
            "step: 005186, mean reward: 184.67\n",
            "step: 005757, mean reward: 142.75\n",
            "step: 006301, mean reward: 108.8\n",
            "step: 006876, mean reward: 82.14\n",
            "step: 007421, mean reward: 77.86\n",
            "step: 007945, mean reward: 87.33\n",
            "step: 008530, mean reward: 97.5\n",
            "step: 009071, mean reward: 135.25\n",
            "step: 009679, mean reward: 121.6\n",
            "step: 010319, mean reward: 160.0\n",
            "step: 010912, mean reward: 148.25\n",
            "step: 011428, mean reward: 129.0\n",
            "step: 012059, mean reward: 157.75\n",
            "step: 012770, mean reward: 237.0\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.wrappers.time_limit import TimeLimit\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "run(\n",
        "    env = TimeLimit(gym.make(env_name), 1000),\n",
        "    max_episodes = 50000,  # количество эпизодов обучения\n",
        "    hidden_size = 64,  # кол-во переменных в скрытых слоях\n",
        "    n_layers=1,\n",
        "    rollout_size = 500,  # через столько шагов стратегия будет обновляться\n",
        "    lr = 0.01, # learning rate\n",
        "    gamma = 0.995,  # дисконтирующий множитель,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efmE8kbTQx8u"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LdAbozcKdpOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}