{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "08b030de",
      "metadata": {
        "id": "08b030de"
      },
      "source": [
        "## Метод Actor-Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3a82f3",
      "metadata": {
        "id": "cb3a82f3"
      },
      "source": [
        "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
        "\n",
        "Встает вопрос, как оценить $Q^\\pi(s, a)$? Ранее в REINFORCE мы использовали отдачу $R_t$ (полученную методом Монте-Карло) в качестве несмещенной оценки $Q^\\pi(s, a)$. В Actor-Critic же предлагается отдельно обучать нейронную сеть Q-функции - критика.\n",
        "\n",
        "Актор-критиком часто называют обобщенный фреймворк (подход), нежели какой-то конкретный алгоритм. Как подход актор-критик не указывает, каким конкретно [policy gradient] методом обучается актор и каким [value based] методом обучается критик. Таким образом актор-критик задает целое семейство различных алгоритмов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4a870514-f49e-4a18-9234-ba682ce1ee07",
      "metadata": {
        "id": "4a870514-f49e-4a18-9234-ba682ce1ee07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81db4b6d-1418-457b-e7d5-c51b62076c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip install \"gymnasium[classic-control, atari, accept-rom-license]\" --quiet\n",
        "    !pip install piglet --quiet\n",
        "    !pip install imageio_ffmpeg --quiet\n",
        "    !pip install moviepy==1.0.3 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b38ba055",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "b38ba055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b5e238-91ef-4cfd-ef6b-95142a232982"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb481bf0",
      "metadata": {
        "id": "bb481bf0"
      },
      "source": [
        "### Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4cedde6d",
      "metadata": {
        "id": "4cedde6d"
      },
      "outputs": [],
      "source": [
        "def print_mean_reward(step, episode_rewards):\n",
        "    if not episode_rewards:\n",
        "        return\n",
        "\n",
        "    t = min(50, len(episode_rewards))    \n",
        "    mean_reward = sum(episode_rewards[-t:]) / t\n",
        "    print(f\"step: {str(step).zfill(6)}, mean reward: {mean_reward:.2f}\")\n",
        "    return mean_reward\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    return x\n",
        "\n",
        "\n",
        "def run(\n",
        "        env: gym.Env, hidden_size: int, n_hidden_layers:int, lr: float, gamma: float, max_episodes: int, \n",
        "        rollout_size: int, replay_buffer_size: int, critic_batch_size: int, critic_updates_per_actor: int\n",
        "):\n",
        "    # Инициализируйте агента `agent`, когда сделаете саму реализацию агента ниже по заданию.\n",
        "    ####### Здесь ваш код ########\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent = ActorCriticAgent(state_dim, action_dim, hidden_size, n_hidden_layers, \n",
        "                             lr, gamma, replay_buffer_size)\n",
        "    ##############################\n",
        "\n",
        "    step = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i_episode in range(1, max_episodes + 1):\n",
        "        cumulative_reward = 0        \n",
        "        terminated = False\n",
        "        state, _ = env.reset()\n",
        "        \n",
        "        while not terminated:\n",
        "            step += 1\n",
        "\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            \n",
        "            agent.append_to_replay_buffer(state, action, reward, next_state, terminated)\n",
        "            state = next_state\n",
        "            cumulative_reward += reward\n",
        "            terminated |= truncated\n",
        "\n",
        "        episode_rewards.append(cumulative_reward)\n",
        "        \n",
        "        # выполняем обновление\n",
        "        if agent.update(rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "            mean_reward = print_mean_reward(step, episode_rewards) \n",
        "            if mean_reward >= 200:\n",
        "                print('Принято!')\n",
        "                return\n",
        "            episode_rewards = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "14d624df",
      "metadata": {
        "id": "14d624df"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "from operator import attrgetter\n",
        "\n",
        "class ActorBatch:\n",
        "    def __init__(self):\n",
        "        self.logprobs = []\n",
        "        self.qvalues = []\n",
        "        \n",
        "    def append(self, log_prob, q_value):\n",
        "        self.logprobs.append(log_prob)\n",
        "        self.qvalues.append(q_value)\n",
        "    \n",
        "    def clear(self):\n",
        "        self.logprobs.clear()\n",
        "        self.qvalues.clear()\n",
        "\n",
        "\n",
        "Transition = namedtuple('Transition', ['loss', 'state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def softmax(self, xs, temp=1.):\n",
        "        if not isinstance(xs, np.ndarray):\n",
        "            xs = np.array(xs, dtype=np.float32)\n",
        "\n",
        "        # Обрати внимание, насколько большая температура по умолчанию!\n",
        "        exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "        return exp_xs / exp_xs.sum()\n",
        "    \n",
        "    def append(self, loss, state, action, reward, next_state, done):\n",
        "        sample = Transition(loss, state, action, reward, next_state, done)\n",
        "        self.buffer.append(sample)\n",
        "\n",
        "    def sample_batch(self, n_samples):\n",
        "        # Sample randomly `n_samples` samples from replay buffer weighting by priority (sample's loss)\n",
        "        # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "        # Also, keep samples' indices (into `indices`) to return them too!\n",
        "\n",
        "        losses = [sample.loss for sample in self.buffer]\n",
        "        probs = self.softmax(losses)\n",
        "        indices = np.random.choice(len(self.buffer), n_samples, p=probs)\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            _, s, a, r, n_s, done = self.buffer[i]\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            next_states.append(n_s)\n",
        "            dones.append(done)\n",
        "\n",
        "        batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "        return batch, indices\n",
        "\n",
        "    def update_batch(self, indices, batch, new_losses):\n",
        "        \"\"\"Updates batches with corresponding indices replacing their loss value.\"\"\"\n",
        "        states, actions, rewards, next_states, is_done = batch\n",
        "\n",
        "        for i in range(len(indices)):\n",
        "            new_sample = Transition(new_losses[i], states[i], actions[i], rewards[i], next_states[i], is_done[i])\n",
        "            self.buffer[indices[i]] = new_sample\n",
        "\n",
        "    def sort(self):\n",
        "        \"\"\"Sorts replay buffer to move samples with lesser loss to the beginning \n",
        "        ==> they will be replaced with the new samples earlier.\"\"\"\n",
        "        new_rb = deque(maxlen=self.buffer.maxlen)\n",
        "        new_rb.extend(sorted(self.buffer, key=attrgetter('loss')))\n",
        "        self.buffer = new_rb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8b058f",
      "metadata": {
        "id": "ac8b058f"
      },
      "source": [
        "Попробуйте сначала реализовать без памяти прецедентов, а затем дополните вашу реализацию. Текущей реализацией приоритизированной памяти достаточно, чтобы пользоваться ей по аналогии с AgentBatch, стоит лишь добавить метод выборки всех данных по аналогии с `sample_batch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "62b2641d",
      "metadata": {
        "id": "62b2641d"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, n_layers):\n",
        "        super().__init__()\n",
        "        act = nn.Tanh()\n",
        "        modules = [nn.Linear(state_dim, hidden_dim), act]\n",
        "        for _ in range(n_layers):\n",
        "            modules.extend([nn.Linear(hidden_dim, hidden_dim), act])\n",
        "        self.net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = to_tensor(state)\n",
        "        return self.net(state)\n",
        "\n",
        "    \n",
        "class ActorCriticModel(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, n_hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Инициализируйте сеть агента с двумя головами: softmax-актора и линейного критика\n",
        "        # self.net, self.actor_head, self.critic_head =\n",
        "        ####### Здесь ваш код ########\n",
        "        self.net = MLPModel(state_dim, hidden_dim, n_hidden_layers)\n",
        "        self.actor_head = nn.Sequential( nn.Linear(hidden_dim, action_dim), nn.Softmax(-1))\n",
        "        self.critic_head = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "        ##############################\n",
        "    def forward(self, state):\n",
        "        # Вычислите выбранное действие, логарифм вероятности его выбора и соответствующее значение Q-функции\n",
        "        ####### Здесь ваш код ########\n",
        "        if len(state.shape) == 1:\n",
        "            state=state[None, :]\n",
        "        xstate = self.net(state)\n",
        "        qvalues = self.critic_head(xstate)\n",
        "        \n",
        "        action_distribution = Categorical(probs = self.actor_head(xstate))\n",
        "        action = action_distribution.sample()\n",
        "        log_prob = action_distribution.log_prob(action)\n",
        "        ##############################\n",
        "        return action.item(), log_prob, qvalues[np.arange(state.shape[0]), action]\n",
        "    \n",
        "    def value_forward(self, state, action = None):\n",
        "        # Вычислите значения Q-функции для данного состояния\n",
        "        ####### Здесь ваш код ########\n",
        "        if len(state.shape) == 1:\n",
        "            state=state[None, :]\n",
        "        xstate = self.net(state)\n",
        "        qvalues = self.critic_head(xstate)\n",
        "\n",
        "        if action is None:\n",
        "            action_distribution = Categorical(probs = self.actor_head(xstate).detach())\n",
        "            action = action_distribution.sample()\n",
        "        ##############################\n",
        "        return qvalues[np.arange(state.shape[0]), action]\n",
        "\n",
        "\n",
        "class ActorCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, hidden_size, n_hidden_layers, lr, gamma, replay_buffer_size):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Инициализируйте модель актор-критика и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
        "        ####### Здесь ваш код ########\n",
        "        self.a2c = ActorCriticModel(state_dim, hidden_size, n_hidden_layers, action_dim).to(device)                          \n",
        "        ##############################\n",
        "        \n",
        "        self.actor_batch = ActorBatch()\n",
        "        self.replay_buffer = PrioritizedReplayBuffer(replay_buffer_size)\n",
        "        \n",
        "    def act(self, state):\n",
        "        # Произведите выбор действия и сохраните необходимые данные в батч для последующего обучения\n",
        "        # Не забудьте сделать q_value.detach()\n",
        "        # self.actor_batch.append(..)\n",
        "        ####### Здесь ваш код ########\n",
        "        action, log_prob, qvalue = self.a2c(state)\n",
        "        self.actor_batch.append(log_prob, qvalue.detach())\n",
        "        ##############################\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def evaluate(self, state):\n",
        "        return self.a2c.value_forward(state)\n",
        "    \n",
        "    def update(self, rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "        if len(self.actor_batch.qvalues) < rollout_size:\n",
        "            return False\n",
        "        \n",
        "        actor_loss = self.update_actor()\n",
        "        critic_loss = self.update_critic(critic_batch_size, critic_updates_per_actor)\n",
        "\n",
        "        loss = actor_loss + actor_loss\n",
        "\n",
        "        # print(f\"losses: Actor={actor_loss} .... Critic={critic_loss}\")\n",
        "        self.actor_batch.clear()\n",
        "        return True\n",
        "\n",
        "    def update_actor(self):\n",
        "        qvalues = torch.stack(self.actor_batch.qvalues).to(device)\n",
        "        logprobs = torch.stack(self.actor_batch.logprobs).to(device)\n",
        "\n",
        "        # Реализуйте шаг обновления актора. Опционально: сделайте нормализацию отдач\n",
        "        ####### Здесь ваш код ########\n",
        "        self.optimizer.zero_grad()\n",
        "        qvalues = qvalues.squeeze(-1)\n",
        "        qvalues = (qvalues - qvalues.mean())/(torch.std(qvalues) +1e-6)\n",
        "        policy_loss = -(qvalues * logprobs).mean() + 1e-3 * (logprobs * torch.exp(logprobs)).sum()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return policy_loss.item()\n",
        "\n",
        "\n",
        "        ##############################\n",
        "    \n",
        "    def update_critic(self, batch_size, critic_updates_per_actor):\n",
        "        # Реализуйте critic_updates_per_actor шагов обучения критика.\n",
        "        ####### Здесь ваш код ########\n",
        "        critic_loss = 0\n",
        "        for _ in range(critic_updates_per_actor):\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            batch, indices = self.replay_buffer.sample_batch(batch_size)\n",
        "            td_loss, td_losses = self.compute_td_loss(*batch)\n",
        "            td_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            critic_loss += td_loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.replay_buffer.update_batch(indices, batch, td_losses.detach().cpu().tolist())\n",
        "    \n",
        "    ##############################\n",
        "\n",
        "        # re-sort replay buffer to prioritize replacing with new samples those samples\n",
        "        # that have the least loss\n",
        "        if len(self.replay_buffer.buffer) >= .75 * (self.replay_buffer.buffer.maxlen):\n",
        "            self.replay_buffer.sort()\n",
        "        \n",
        "\n",
        "        return critic_loss/critic_updates_per_actor\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def append_to_replay_buffer(self, s, a, r, next_s, done):\n",
        "        # Добавьте новый экземпляр данных в память прецедентов.\n",
        "        ####### Здесь ваш код ########\n",
        "        _, losses = self.compute_td_loss([s], [a], [r], [next_s], [done])\n",
        "        self.replay_buffer.append(losses.cpu().tolist()[0], s, a, r, next_s, done)\n",
        "        ##############################\n",
        "        \n",
        "    def compute_td_loss(\n",
        "        self, states, actions, rewards, next_states, is_done, regularizer=.1\n",
        "    ):\n",
        "        \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch\"\"\"\n",
        "\n",
        "        # переводим входные данные в тензоры\n",
        "        states = to_tensor(states)                      # shape: [batch_size, state_size]\n",
        "        actions = to_tensor(actions, int).long()        # shape: [batch_size]\n",
        "        rewards = to_tensor(rewards)                    # shape: [batch_size]\n",
        "        next_states = to_tensor(next_states)            # shape: [batch_size, state_size]\n",
        "        is_done = to_tensor(is_done, bool)              # shape: [batch_size]\n",
        "\n",
        "        # Реализуйте шаг обновления критика\n",
        "        ####### Здесь ваш код ########\n",
        "\n",
        "        state_values = self.evaluate(states).squeeze(-1)\n",
        "        next_state_values = self.evaluate(next_states).squeeze(-1).detach()\n",
        "        \n",
        "        td_losses = (rewards + self.gamma * next_state_values * (~is_done) - state_values)**2\n",
        "        td_loss = torch.mean(td_losses) + regularizer * next_state_values.mean()\n",
        "        ##############################\n",
        "        return td_loss, td_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3366c97f",
      "metadata": {
        "id": "3366c97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f19c9e-5281-4d95-89ab-5681a23deafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 000515, mean reward: 23.41\n",
            "step: 001019, mean reward: 10.50\n",
            "step: 001524, mean reward: 10.52\n",
            "step: 002024, mean reward: 9.66\n",
            "step: 002527, mean reward: 15.24\n",
            "step: 003028, mean reward: 10.02\n",
            "step: 003620, mean reward: 197.33\n",
            "step: 004136, mean reward: 64.50\n",
            "step: 004648, mean reward: 15.52\n",
            "step: 005152, mean reward: 19.38\n",
            "step: 005659, mean reward: 12.68\n",
            "step: 006169, mean reward: 46.36\n",
            "step: 006672, mean reward: 20.12\n",
            "step: 007179, mean reward: 11.79\n",
            "step: 007685, mean reward: 9.36\n",
            "step: 008188, mean reward: 22.86\n",
            "step: 008696, mean reward: 9.78\n",
            "step: 009199, mean reward: 11.43\n",
            "step: 009705, mean reward: 10.33\n",
            "step: 010304, mean reward: 119.80\n",
            "step: 010806, mean reward: 13.21\n",
            "step: 011308, mean reward: 10.24\n",
            "step: 011819, mean reward: 26.89\n",
            "step: 012327, mean reward: 9.96\n",
            "step: 012827, mean reward: 500.00\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.wrappers.time_limit import TimeLimit\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "run(\n",
        "    env = TimeLimit(gym.make(env_name), 1000),\n",
        "    max_episodes = 50000,  # количество эпизодов обучения\n",
        "    hidden_size = 64,  # кол-во переменных в скрытых слоях\n",
        "    rollout_size = 500,  # через столько шагов стратегия будет обновляться\n",
        "    lr = 0.01, # learning rate\n",
        "    n_hidden_layers = 1,\n",
        "    gamma = 0.995,  # дисконтирующий множитель,\n",
        "    replay_buffer_size = 5000,\n",
        "    critic_batch_size = 64,\n",
        "    critic_updates_per_actor = 32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4977dd5e",
      "metadata": {
        "id": "4977dd5e"
      },
      "outputs": [],
      "source": [
        "# https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppg_procgen.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KkC4bseQYrEa"
      },
      "id": "KkC4bseQYrEa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}