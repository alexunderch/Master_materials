{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIFTFwVzn4HH"
      },
      "source": [
        "## Proximal Policy Optimization \n",
        "\n",
        "В практической реализации существует два варианта реализации алгоритма PPO:\n",
        "* выполняет обновление, ограниченное KL, как TRPO, но штрафует KL-расхождение в целевой функции вместо того, чтобы делать его жестким ограничением, и автоматически регулирует коэффициент штрафа в процессе обучения, чтобы он масштабировался соответствующим образом.\n",
        "* не содержит в целевой функции члена KL-дивергенции и вообще не имеет ограничения. Вместо этого полагается на специализированный клиппинг \n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg\">\n",
        "\n",
        "Спойлер: клиппинг - не самое главное в PPO, как это могло показаться на первый взгляд. Алгоритм PPO работает во многом и за счет небольших дополнительных улучшений. Подробнее: https://arxiv.org/pdf/2005.12729.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NsANtYZn4HJ"
      },
      "source": [
        "### Задание 1: Заполните пропуски в алгоритме"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g5AehNkpn4HK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ca85d1-1b63-4438-84b9-ba004c261b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 KB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 1,086 kB of archives.\n",
            "After this operation, 5,413 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1,081 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5,528 B]\n",
            "Fetched 1,086 kB in 1s (903 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 128285 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.1-5build1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\n",
            "Unpacking swig (4.0.1-5build1) ...\n",
            "Setting up swig4.0 (4.0.1-5build1) ...\n",
            "Setting up swig (4.0.1-5build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.9/dist-packages (0.28.0)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (6.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (1.22.4)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (2.1.3)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.15.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp39-cp39-linux_x86_64.whl size=2778660 sha256=d2436589498162fd6e2f4afc560d292ab74a3fab9bd5d7ff586e045b6f6aebda\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/c2/c1/076651c394f05fe60990cd85616c2d95bc1619aa113f559d7d\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.1.1\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "print(COLAB)\n",
        "if COLAB:\n",
        "    !pip install \"gymnasium[classic-control, atari, accept-rom-license]\" --quiet\n",
        "    !pip install piglet --quiet\n",
        "    !pip install imageio_ffmpeg --quiet\n",
        "    !pip install moviepy==1.0.3 --quiet\n",
        "    !apt-get install swig -y\n",
        "    !pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5wFRbSCmn4HL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IX9DwL1vn4HL"
      },
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GejGf_Sn4HM"
      },
      "source": [
        "### Сеть Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NnWk5m3Ln4HM"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # actor: 2 hidden + output\n",
        "        # self.action_layer = nn.Sequential(..., nn.Softmax(dim=-1))\n",
        "        ####### Здесь ваш код ########\n",
        "        self.action_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(-1)\n",
        "        )\n",
        "        ##############################\n",
        "\n",
        "        # critic: 2 hidden + output\n",
        "        # self.value_layer = nn.Sequential(...)\n",
        "        ####### Здесь ваш код ########\n",
        "        self.value_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        ##############################\n",
        "\n",
        "    def forward(self):\n",
        "        action_probs = self.action_layer(state)\n",
        "        state_value = self.value_layer(state)\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        \n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        \n",
        "        # сохраняем в память: state, action, log_prob(action)\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIwcG9zLn4HN"
      },
      "source": [
        "### PPO policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yqjY9osjn4HN"
      },
      "outputs": [],
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo оценка вознаграждений:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            # обнуляем накопленную награду, если попали в терминальное состояние\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            # discounted_reward = \n",
        "            ####### Здесь ваш код ########\n",
        "            discounted_reward += self.gamma * reward\n",
        "            ##############################\n",
        "            rewards.append(discounted_reward)\n",
        "\n",
        "        rewards = torch.tensor(rewards[::-1], dtype=torch.float32).to(device)\n",
        "        # выполните нормализацию вознаграждений (r - mean(r)) / std(r + 1e-5):\n",
        "        ####### Здесь ваш код ########\n",
        "        rewards = (rewards - rewards.mean()) / (torch.std(rewards) + 1e-6) \n",
        "        ##############################\n",
        "        \n",
        "        # конвертация list в tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "\n",
        "        # оптимизация K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # получаем logprobs, state_values, dist_entropy от старой стратегии:\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "\n",
        "            # находим отношение стратегий (pi_theta / pi_theta_old), через logprobs и old_logprobs.detach():\n",
        "            # ratios = \n",
        "            ####### Здесь ваш код ########\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "            ##############################\n",
        "    \n",
        "            # считаем advantages\n",
        "            # advantages = \n",
        "            ####### Здесь ваш код ########\n",
        "            advantages = (rewards - state_values).detach()\n",
        "            ##############################\n",
        "            \n",
        "            # Находим surrogate loss:\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.loss(state_values, rewards) - 0.01 * dist_entropy\n",
        "            \n",
        "            # делаем шаг градиента\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # копируем веса\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gx4PGJPn4HO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKw0O3cen4HP"
      },
      "source": [
        "### Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KabBBEVun4HP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4be1984-e764-4637-8afa-4382cfa211db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20 \t avg length: 88 \t reward: -196\n",
            "Episode 40 \t avg length: 103 \t reward: -223\n",
            "Episode 60 \t avg length: 86 \t reward: -167\n",
            "Episode 80 \t avg length: 79 \t reward: -132\n",
            "Episode 100 \t avg length: 95 \t reward: -168\n",
            "Episode 120 \t avg length: 79 \t reward: -110\n",
            "Episode 140 \t avg length: 83 \t reward: -135\n",
            "Episode 160 \t avg length: 92 \t reward: -120\n",
            "Episode 180 \t avg length: 88 \t reward: -106\n",
            "Episode 200 \t avg length: 101 \t reward: -109\n",
            "Episode 220 \t avg length: 96 \t reward: -102\n",
            "Episode 240 \t avg length: 105 \t reward: -96\n",
            "Episode 260 \t avg length: 99 \t reward: -65\n",
            "Episode 280 \t avg length: 116 \t reward: -67\n",
            "Episode 300 \t avg length: 96 \t reward: -63\n",
            "Episode 320 \t avg length: 107 \t reward: -61\n",
            "Episode 340 \t avg length: 132 \t reward: -72\n",
            "Episode 360 \t avg length: 114 \t reward: -47\n",
            "Episode 380 \t avg length: 138 \t reward: -34\n",
            "Episode 400 \t avg length: 164 \t reward: -57\n",
            "Episode 420 \t avg length: 153 \t reward: -25\n",
            "Episode 440 \t avg length: 130 \t reward: -31\n",
            "Episode 460 \t avg length: 132 \t reward: -11\n",
            "Episode 480 \t avg length: 191 \t reward: 17\n",
            "Episode 500 \t avg length: 290 \t reward: 12\n",
            "Episode 520 \t avg length: 283 \t reward: -27\n",
            "Episode 540 \t avg length: 299 \t reward: 9\n",
            "Episode 560 \t avg length: 296 \t reward: 5\n",
            "Episode 580 \t avg length: 329 \t reward: 41\n",
            "Episode 600 \t avg length: 241 \t reward: 52\n",
            "Episode 620 \t avg length: 372 \t reward: 51\n",
            "Episode 640 \t avg length: 409 \t reward: 62\n",
            "Episode 660 \t avg length: 317 \t reward: 32\n",
            "Episode 680 \t avg length: 333 \t reward: 48\n",
            "Episode 700 \t avg length: 284 \t reward: 15\n",
            "Episode 720 \t avg length: 248 \t reward: -77\n",
            "Episode 740 \t avg length: 330 \t reward: -23\n",
            "Episode 760 \t avg length: 347 \t reward: -77\n",
            "Episode 780 \t avg length: 354 \t reward: 68\n",
            "Episode 800 \t avg length: 281 \t reward: 43\n",
            "Episode 820 \t avg length: 283 \t reward: 50\n",
            "Episode 840 \t avg length: 294 \t reward: 46\n",
            "Episode 860 \t avg length: 388 \t reward: 92\n",
            "Episode 880 \t avg length: 347 \t reward: 87\n",
            "Episode 900 \t avg length: 350 \t reward: 57\n",
            "Episode 920 \t avg length: 299 \t reward: 64\n",
            "Episode 940 \t avg length: 337 \t reward: 76\n",
            "Episode 960 \t avg length: 389 \t reward: 97\n",
            "Episode 980 \t avg length: 450 \t reward: 100\n",
            "Episode 1000 \t avg length: 347 \t reward: 71\n",
            "Episode 1020 \t avg length: 250 \t reward: 67\n",
            "Episode 1040 \t avg length: 349 \t reward: 89\n",
            "Episode 1060 \t avg length: 314 \t reward: 81\n",
            "Episode 1080 \t avg length: 227 \t reward: 51\n",
            "Episode 1100 \t avg length: 127 \t reward: 14\n",
            "Episode 1120 \t avg length: 276 \t reward: 70\n",
            "Episode 1140 \t avg length: 429 \t reward: 106\n",
            "Episode 1160 \t avg length: 432 \t reward: 98\n",
            "Episode 1180 \t avg length: 433 \t reward: 87\n",
            "Episode 1200 \t avg length: 427 \t reward: 102\n",
            "Episode 1220 \t avg length: 434 \t reward: 84\n",
            "Episode 1240 \t avg length: 463 \t reward: 98\n",
            "Episode 1260 \t avg length: 435 \t reward: 99\n",
            "########## Принято! ##########\n"
          ]
        }
      ],
      "source": [
        "# env_name = \"CartPole-v1\"\n",
        "env_name = \"LunarLander-v2\"\n",
        "# env_name = \"MountainCar-v0\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "render = False\n",
        "\n",
        "# for LunarLander-v2 max reward is cut by 140 as stated in docs to dont relax no more\n",
        "# source: https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
        "solved_reward = 110  # останавливаемся если avg_reward > solved_reward\n",
        "\n",
        "log_interval = 20  # печатаем avg reward  в интервале \n",
        "max_episodes = 50000  # количество эпизодов обучения\n",
        "max_timesteps = 500  # максимальное кол-во шагов в эпизоде\n",
        "n_latent_var = 128  # кол-во переменных в скрытых слоях\n",
        "update_timestep = 1000  # обновляем policy каждые n шагов\n",
        "lr = 0.001 # learning rate\n",
        "betas = (0.9, 0.999) # betas для adam optimizer\n",
        "gamma = 0.99  # discount factor\n",
        "K_epochs = 4  # количество эпох обноеления policy\n",
        "eps_clip = 0.1  # clip параметр для PPO\n",
        "random_seed = None\n",
        "\n",
        "if random_seed:\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "\n",
        "memory = Memory()\n",
        "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
        "\n",
        "# переменные для логирования\n",
        "running_reward = 0\n",
        "avg_length = 0\n",
        "timestep = 0\n",
        "\n",
        "# цикл обучения\n",
        "for i_episode in range(1, max_episodes + 1):\n",
        "    state, _ = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "        timestep += 1\n",
        "\n",
        "        # используем policy_old для выбора действия\n",
        "        action = ppo.policy_old.act(state, memory)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # сохраняем награды и флаги терминальных состояний:\n",
        "        memory.rewards.append(reward)\n",
        "        memory.is_terminals.append(terminated)\n",
        "\n",
        "        # выполняем обновление\n",
        "        if timestep % update_timestep == 0:\n",
        "            ppo.update(memory)\n",
        "            memory.clear_memory()\n",
        "            timestep = 0\n",
        "\n",
        "        running_reward += reward\n",
        "        if render:\n",
        "            env.render()\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    avg_length += t\n",
        "\n",
        "    # останавливаемся, если avg_reward > solved_reward\n",
        "    if running_reward > (log_interval * solved_reward):\n",
        "        print(\"########## Принято! ##########\")\n",
        "        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n",
        "        break\n",
        "\n",
        "    # логирование\n",
        "    if i_episode % log_interval == 0:\n",
        "        avg_length = int(avg_length / log_interval)\n",
        "        running_reward = int((running_reward / log_interval))\n",
        "\n",
        "        print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "        running_reward = 0\n",
        "        avg_length = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwNg4WrAn4HP"
      },
      "source": [
        "### Задание 2: Попробуйте обучить алгоритм PPO, используя более сложную среду (например LunarLander-v2), для чего вам потребуется подобрать некотороые гиперпараметры.\n",
        "\n",
        "**Работа со средой LunarLander в colab - установка pybox2d:**\n",
        "1. Устанавливаем пакеты: \n",
        "```bash\n",
        "!apt-get install swig -y\n",
        "!pip install box2d-py\n",
        "```\n",
        "2. Перезапускаем runtime."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYHfpf6elxUx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}