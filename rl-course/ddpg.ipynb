{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cjehRbPEVRh"
      },
      "source": [
        "## Deep Deterministic Policy Gradient\n",
        "\n",
        "На этом семинаре мы будем обучать нейронную сеть на фреймворке __pytorch__ с помощью алгоритма Deep Deterministic Policy Gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYHGD-j9EVRi"
      },
      "source": [
        "## Теория\n",
        "\n",
        "Deep Deterministic Policy Gradient (DDPG) - это алгоритм, который одновременно учит Q-функцию и стратегию. Он использует off-policy данные и уравнения Беллмана для обучения Q-функции, а Q-функция используется для обучения стратегии.\n",
        "\n",
        "Данный подход тесно связан с Q-обучением и мотивирован следующей идеей: если вы знаете оптимальную функцию action-value $Q^*(s,a)$, тогда для конкретного состояния, оптимальное действие $a^*(s)$ может быть найдено решением: \n",
        "\n",
        "$$a^*(s) = \\arg \\max_a Q^*(s,a).$$\n",
        "\n",
        "Для сред с дискретным пространством действий - это легко, вычисляем полезности для каждого из действий, а потом берем максимум. Для непрерывных действий - это сложная оптимизационная задача.\n",
        "\n",
        "DDPG чередует обучение аппроксиматора $Q^*(s,a)$ с обучением аппроксиматора  $a^*(s)$, и делает это специальным образом именно для непрерывных (continuous) сред, что отражается в том как алгоритм вычисляет $\\max_a Q^*(s,a)$.\n",
        "Поскольку пространство действий непрерывно, предполагается, что функция $Q^*(s,a)$ дифференцируема по аргументу действия. Это позволяет нам установить эффективное правило обучения на основе градиента для стратегии $\\mu(s)$.\n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/5811066e89799e65be299ec407846103fcf1f746.svg\">\n",
        "\n",
        "Оригинальная статья:  <a href=\"https://arxiv.org/abs/1509.02971\">Continuous control with deep reinforcement learning Arxiv</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJQNAIKkgoEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe65711-75df-4b68-e9b6-934f1a93e426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 KB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip install \"gymnasium[classic-control, atari, accept-rom-license]\" --quiet\n",
        "    !pip install piglet --quiet\n",
        "    !pip install imageio_ffmpeg --quiet\n",
        "    !pip install moviepy==1.0.3 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RTdvHHw8goEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7dd430-4bf5-4c78-cb47-ea713bc9fe50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15fnzjTPgoEs"
      },
      "source": [
        "### Вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-oLgI-kgoEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78381b1-3e99-41ee-f8bb-a4ba643524e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def print_mean_reward(step, session_rewards, eval_session_rewards):\n",
        "    if not session_rewards:\n",
        "        return\n",
        "\n",
        "    def get_mean_reward(rewards): \n",
        "        return round(sum(rewards) / len(rewards), 2)\n",
        "    \n",
        "    train_mean = get_mean_reward(session_rewards)\n",
        "    eval_mean = get_mean_reward(eval_session_rewards)\n",
        "    \n",
        "    print(f\"step: {str(step).zfill(6)}, train: {train_mean}, eval: {eval_mean}\")\n",
        "    return eval_mean\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxqM-qYYgoEu"
      },
      "source": [
        "### Batch/replay buffer\n",
        "\n",
        "Попробуйте сначала реализовать с обычным буфером для накопления прецедентов с момента последнего шага обновления агента. Затем там, где это возможно, дополните вашу реализацию более серьезной памятью прецедентов (опционально добавьте приоритизацию).\n",
        "Задайте себе вопрос, можно ли использовать память прецедентов для обучения актора в DDPG? Является ли алгоритм DDPG off-policy алгоритмом и почему?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYhc24xlgoEu"
      },
      "outputs": [],
      "source": [
        "####### Здесь ваш код ########\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer.clear()\n",
        "        self.position = 0\n",
        "\n",
        "##############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MWmwi9vEVRk",
        "tags": []
      },
      "source": [
        "## Environment\n",
        "### Нормализация пространства действий"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avcobSjtEVRk"
      },
      "outputs": [],
      "source": [
        "class NormalizedActions(gym.ActionWrapper):\n",
        "    def action(self, action: np.ndarray) -> np.ndarray:\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "\n",
        "        scale_factor = (high - low) / 2\n",
        "        reloc_factor = high - scale_factor\n",
        "\n",
        "        action = action * scale_factor + reloc_factor\n",
        "        action = np.clip(action, low, high)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "\n",
        "        scale_factor = (high - low) / 2\n",
        "        reloc_factor = high - scale_factor\n",
        "\n",
        "        action = (action - reloc_factor) / scale_factor\n",
        "        action = np.clip(action, -1.0, 1.0)\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vlzBWF6EVRk"
      },
      "source": [
        "### Исследование - GaussNoise\n",
        "#### Добавляем Гауссовский шум к действиям детерминированной стратегии\n",
        "Добавляем его только при обучении для исследования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JZ9_FRdEVRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c112c139-f65d-49ce-db4d-e30ba6051f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class GaussNoise:\n",
        "    def __init__(self, mu: float, sigma: float):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dist = torch.distributions.Normal(self.mu, self.sigma)\n",
        "\n",
        "    def sample(self, shape: torch.Size):\n",
        "        noisy_action = self.dist.sample(shape).to(device)\n",
        "        return noisy_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5qA-jz-EVRk"
      },
      "source": [
        "## DDPG Network \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN-DDPG.svg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIBeomkAgoEx"
      },
      "source": [
        "### DDPG Model\n",
        "Реализуйте модель актор-критика `DdpgModel`. Скорее всего вам понадобится `nn.Module`. Можете реализовать актор-критика единым модулем, а можете разнести их (первый вариант не обязательно предполагает общее тело сетей)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of6B9s6PgoEx"
      },
      "outputs": [],
      "source": [
        "class DDPGModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_inputs,\n",
        "            num_actions,\n",
        "            hidden_size,\n",
        "            noise_params: dict\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.value_net = nn.Sequential(\n",
        "            nn.Linear(num_inputs + num_actions, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        \n",
        "        self.policy_net = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, num_actions),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.noise_model = GaussNoise(**noise_params)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return (self.value_forward(state, action), \n",
        "               self.policy_forward(state).detach().cpu().numpy()[0])\n",
        "\n",
        "    def value_forward(self, state, action):\n",
        "        state = to_tensor(state); action = to_tensor(action)\n",
        "\n",
        "        if len(state.shape) == 1:\n",
        "            state=state.unsqueeze(0)\n",
        "\n",
        "        if len(action.shape) == 1:\n",
        "            action=action.unsqueeze(0)\n",
        "        \n",
        "        x = torch.cat([state, action], -1)\n",
        "        value = self.value_net(x)\n",
        "        return value\n",
        "\n",
        "    def policy_forward(self, state):\n",
        "        \n",
        "         state = to_tensor(state)\n",
        "         if len(state.shape) == 1:\n",
        "            state=state.unsqueeze(0)\n",
        "         action = self.policy_net(state)\n",
        "         \n",
        "         if self.training:\n",
        "            noise = self.noise_model.sample(action.size())\n",
        "            selected_action = torch.clip(action + noise, min=-1.0, max=1.0) \n",
        "\n",
        "         return action\n",
        "\n",
        "    def soft_copy(self, other, v_tau: float, p_tau: float):\n",
        "        for target_param, param in zip(other.policy_net.parameters(), self.policy_net.parameters()):\n",
        "            target_param.data.copy_(\n",
        "                target_param.data * (1.0 - p_tau) + param.data * p_tau\n",
        "            )\n",
        "        for target_param, param in zip(other.value_net.parameters(), self.value_net.parameters()):\n",
        "            target_param.data.copy_(\n",
        "                target_param.data * (1.0 - v_tau) + param.data * v_tau\n",
        "            )\n",
        "        return self\n",
        "\n",
        "##############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZROPaX4goEy"
      },
      "source": [
        "### DDPG Agent\n",
        "\n",
        "Реализуйте класс агента `DdpgAgent`, который содержит:\n",
        "\n",
        "- обучаемую модель актор-критика и его оптимизаторы (например, `torch.optim.Adam`)\n",
        "- периодически обновляемую копию модели с замороженными весами для вычисления TD target.\n",
        "- метод `act` для выбора действия. Можете добавить флаг `learn` для подмешивания гауссовского шума при обучении, либо добавляйте его снаружи агента.\n",
        "- метод `learn` для обновления модели агента (и актора, и критика) на новом пакете опыта."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld8fQ0eVgoEy"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "class DDPGAgent:\n",
        "    def __init__(self, \n",
        "                 state_dim: int, \n",
        "                 hidden_dim: int, \n",
        "                 action_dim: int, \n",
        "                 lr: dict, \n",
        "                 gamma: float, \n",
        "                 taus: dict,\n",
        "                 noise_params: dict):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.tau = taus\n",
        "\n",
        "        self.agent = DDPGModel(state_dim, action_dim, hidden_dim, noise_params).to(device)\n",
        "        self.target_net = DDPGModel(state_dim, action_dim, hidden_dim, noise_params).to(device).soft_copy(self.agent, 1., 1.)\n",
        "        self.value_optimizer = torch.optim.Adam([\n",
        "            {\"params\": self.agent.value_net.parameters(), \"lr\": self.lr[\"valuef\"]}])\n",
        "        \n",
        "        self.policy_optimizer =  torch.optim.Adam([ \n",
        "            {\"params\": self.agent.policy_net.parameters(), \"lr\": self.lr[\"policy\"]}])\n",
        "        \n",
        "        self.min_max_value = {\"min\": -np.inf, \"max\": np.inf}    \n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "    def act(self, state, action):\n",
        "        ####### Здесь ваш код ########\n",
        "        value, action = self.agent(state, action)\n",
        "        return value, action\n",
        "        ##############################\n",
        "\n",
        "    def get_action(self, state):\n",
        "        return self.agent.policy_forward(state).item()\n",
        "\n",
        "    def update(self, batched_rollout: tuple):\n",
        "        state, action, reward, next_state, terminal = batched_rollout\n",
        "        action = to_tensor(action).reshape(-1, 1)\n",
        "        reward = to_tensor(reward).reshape(-1, 1)\n",
        "        terminal = to_tensor(terminal, dtype=np.int8).reshape(-1, 1)\n",
        "\n",
        "\n",
        "        policy_loss = self.agent.value_forward(state, self.agent.policy_forward(state))\n",
        "        policy_loss = -policy_loss.mean()\n",
        "        \n",
        "        next_action = self.target_net.policy_forward(next_state)\n",
        "        target_next_value = self.target_net.value_forward(next_state, next_action.detach())\n",
        "        ####### Здесь ваш код ########\n",
        "        target = reward + self.gamma * (1. - terminal) * target_next_value\n",
        "        ##############################\n",
        "        \n",
        "        ####### Здесь ваш код ########\n",
        "        current_value = self.agent.value_forward(state, action)\n",
        "        ##############################\n",
        "    \n",
        "        ####### Здесь ваш код ########\n",
        "        value_loss = nn.MSELoss(reduction = \"mean\")(current_value, target.detach())\n",
        "        ##############################\n",
        "        \n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "\n",
        "        self.value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.value_optimizer.step()\n",
        "        \n",
        "        self.agent = self.agent.soft_copy(self.target_net, self.tau[\"valuef\"], self.tau[\"policy\"])\n",
        "##############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTxCZQQxgoEy"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Реализуйте функцию `run`, которая принимает среду, гиперпараметры агента и условие останова эксперимента (return threshold $G_{target}$). Используйте функцию `print_mean_reward` для вывода промежуточных результатов качества агента в трейн и eval режимах.\n",
        "\n",
        "Проведите эксперимент на среде с непрерывным пространством действий (например, continuous montain car или pendulum)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class ExperimentReplay:\n",
        "    agent: DDPGAgent\n",
        "    replay_buffer: ReplayBuffer\n",
        "    batch_size: int\n",
        "    train_schedule: int\n",
        "\n",
        "    def generate_session(self, T: int, train: bool = False) -> int:\n",
        "        total_reward = 0\n",
        "        state, _ = env.reset()\n",
        "        self.agent.agent.train(train)\n",
        "        for t in range(T):\n",
        "            action = self.agent.get_action(state)\n",
        "            next_state, reward, terminated, info = env.step(action)\n",
        "            # terminated |= truncated\n",
        "            if train:\n",
        "                self.replay_buffer.push(state, action, reward, next_state, terminated)\n",
        "                if len(self.replay_buffer) > self.batch_size and t % self.train_schedule == 0:\n",
        "                    states, actions, rewards, next_states, terminals = self.replay_buffer.sample(self.batch_size)\n",
        "                    self.agent.update((states, actions, rewards, next_states, terminals))\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            if terminated:\n",
        "                break\n",
        "\n",
        "        return total_reward"
      ],
      "metadata": {
        "id": "6M859NldBz8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS9D0g6WgoEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b71a119-05ec-4120-839f-8e09f359d0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch #00\tmean reward (train) = -40.327\tmean reward (valid) = -48.810\n",
            "epoch #01\tmean reward (train) = -48.412\tmean reward (valid) = -38.473\n",
            "epoch #02\tmean reward (train) = -9.119\tmean reward (valid) = -0.912\n",
            "epoch #03\tmean reward (train) = -0.626\tmean reward (valid) = -0.203\n",
            "epoch #04\tmean reward (train) = -0.095\tmean reward (valid) = -0.088\n",
            "epoch #05\tmean reward (train) = -0.091\tmean reward (valid) = -0.102\n",
            "Pendulum решен!\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gym.wrappers import TimeLimit\n",
        "# env_name, success_reward = \"MountainCarContinuous\", 100\n",
        "env_name, success_reward = \"Pendulum-v0\", -100\n",
        "env = NormalizedActions(TimeLimit(gym.make(\"MountainCarContinuous-v0\"), 1000))\n",
        "\n",
        "noise_params = {\"mu\": 0.01, \"sigma\": 0.3}\n",
        "lrs = {\"valuef\": 1e-3, \"policy\": 1e-4}\n",
        "taus = {\"valuef\": .8, \"policy\": .85}\n",
        "batch_size = 100\n",
        "gamma = 0.99\n",
        "state_dim  = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "hidden_dim = 256\n",
        "max_steps = 500\n",
        "\n",
        "agent = DDPGAgent(state_dim=state_dim, \n",
        "                 hidden_dim=hidden_dim, \n",
        "                 action_dim=action_dim, \n",
        "                 lr = lrs, \n",
        "                 gamma = 0.99, \n",
        "                 taus = taus,\n",
        "                 noise_params = noise_params)\n",
        "\n",
        "memory = ReplayBuffer(1000)\n",
        "exp= ExperimentReplay(agent = agent, replay_buffer=memory, batch_size =batch_size, train_schedule = 16)\n",
        "\n",
        "valid_mean_rewards = []\n",
        "for i in range(100):    \n",
        "    session_rewards_train = [\n",
        "        exp.generate_session(max_steps, train=True) \n",
        "        for _ in range(10)\n",
        "    ]\n",
        "    session_rewards_valid = [\n",
        "        exp.generate_session(max_steps, train=False) \n",
        "        for _ in range(10)\n",
        "    ]\n",
        "    print(\n",
        "        \"epoch #{:02d}\\tmean reward (train) = {:.3f}\\tmean reward (valid) = {:.3f}\".format(\n",
        "        i, np.mean(session_rewards_train), np.mean(session_rewards_valid))\n",
        "    )\n",
        "\n",
        "    valid_mean_rewards.append(np.mean(session_rewards_valid))\n",
        "    if len(valid_mean_rewards) > 5 and np.mean(valid_mean_rewards[-5:]) > success_reward:\n",
        "        print(\"Pendulum решен!\")\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xK2UFiS7GnRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}